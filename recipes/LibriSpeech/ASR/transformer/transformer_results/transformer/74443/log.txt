2024-01-18 09:24:38,155 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:24:38,155 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:24:38,543 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:24:40,139 - librispeech_prepare - INFO - Data_preparation...
2024-01-18 09:24:40,140 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 453, in <module>
    run_on_main(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/distributed.py", line 60, in run_on_main
    func(*args, **kwargs)
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 113, in prepare_librispeech
    check_librispeech_folders(data_folder, splits)
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 458, in check_librispeech_folders
    raise OSError(err_msg)
OSError: the folder /home/rajivratn/satyam/LibriSpeech/train-other-500 does not exist (it is expected in the Librispeech dataset)
2024-01-18 09:25:05,825 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:25:05,825 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:25:06,203 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:25:07,680 - librispeech_prepare - INFO - Data_preparation...
2024-01-18 09:25:07,681 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 453, in <module>
    run_on_main(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/distributed.py", line 60, in run_on_main
    func(*args, **kwargs)
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 113, in prepare_librispeech
    check_librispeech_folders(data_folder, splits)
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 458, in check_librispeech_folders
    raise OSError(err_msg)
OSError: the folder /home/rajivratn/satyam/LibriSpeech/train-other-500 does not exist (it is expected in the Librispeech dataset)
2024-01-18 09:26:30,475 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:26:30,477 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:26:30,866 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:26:32,776 - librispeech_prepare - INFO - Data_preparation...
2024-01-18 09:26:34,691 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/train-clean-100.csv...
2024-01-18 09:26:45,628 - librispeech_prepare - INFO - results/transformer/74443/train-clean-100.csv successfully created!
2024-01-18 09:26:48,257 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/train-clean-360.csv...
2024-01-18 09:27:06,768 - librispeech_prepare - INFO - results/transformer/74443/train-clean-360.csv successfully created!
2024-01-18 09:27:06,864 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/dev-clean.csv...
2024-01-18 09:27:13,918 - librispeech_prepare - INFO - results/transformer/74443/dev-clean.csv successfully created!
2024-01-18 09:27:13,974 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/test-clean.csv...
2024-01-18 09:27:21,561 - librispeech_prepare - INFO - results/transformer/74443/test-clean.csv successfully created!
2024-01-18 09:27:21,623 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/test-other.csv...
2024-01-18 09:27:29,845 - librispeech_prepare - INFO - results/transformer/74443/test-other.csv successfully created!
2024-01-18 09:27:29,987 - speechbrain.dataio.dataio - INFO - results/transformer/74443/train.csv is created.
2024-01-18 09:27:30,861 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 09:27:30,861 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Delegating to Huggingface hub, source speechbrain/asr-transformer-transformerlm-librispeech.
2024-01-18 09:27:30,907 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2024-01-18 09:27:31,603 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /speechbrain/asr-transformer-transformerlm-librispeech/resolve/main/lm.ckpt HTTP/1.1" 302 0
2024-01-18 09:27:31,607 - filelock - DEBUG - Attempting to acquire lock 140300826656912 on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/87f1398c0ed833631e487aedfbda32be4c9618565482f6fad78ca4e8dda03e5b.lock
2024-01-18 09:27:31,607 - filelock - DEBUG - Lock 140300826656912 acquired on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/87f1398c0ed833631e487aedfbda32be4c9618565482f6fad78ca4e8dda03e5b.lock
2024-01-18 09:27:31,609 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443
2024-01-18 09:27:32,465 - urllib3.connectionpool - DEBUG - https://cdn-lfs.huggingface.co:443 "GET /speechbrain/asr-transformer-transformerlm-librispeech/87f1398c0ed833631e487aedfbda32be4c9618565482f6fad78ca4e8dda03e5b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27lm.ckpt%3B+filename%3D%22lm.ckpt%22%3B&Expires=1705829251&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTgyOTI1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zcGVlY2hicmFpbi9hc3ItdHJhbnNmb3JtZXItdHJhbnNmb3JtZXJsbS1saWJyaXNwZWVjaC84N2YxMzk4YzBlZDgzMzYzMWU0ODdhZWRmYmRhMzJiZTRjOTYxODU2NTQ4MmY2ZmFkNzhjYTRlOGRkYTAzZTViP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=HmVcrpm-bUtR0O9j0myaZQaBQBl-zwsAvFOpTLsA4UqoK8mUL2zsPS0vmnKJTaKEjpcqlrmGzdPJSXR-1gcUfKJrw9UKRnfrABMO8LNMdj2fqU9fDD3ShcOFbmPzvX45abmSdg5ZK2AeX7LFI-g2oUHjyEaUMkmpwptTaHcT4wgCgH1JvggfMdzhtm4i1EBOASNTJVcQcvHfiug32cEzdE0IpVqNhh9kSpU6MsXJ1zuSGPYS~kES62DPQIw4OPAi0WnmmbwDxY5WIJVMCNDCHkc~Wv-obthBIcWLiwUhzBbidp3hmT7SN4ydcNqxSewgVoHJvikweVSOcCsV2FGZRQ__&Key-Pair-Id=KVTP0A1DKRTAX HTTP/1.1" 200 381074869
2024-01-18 09:27:35,855 - filelock - DEBUG - Attempting to release lock 140300826656912 on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/87f1398c0ed833631e487aedfbda32be4c9618565482f6fad78ca4e8dda03e5b.lock
2024-01-18 09:27:35,855 - filelock - DEBUG - Lock 140300826656912 released on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/87f1398c0ed833631e487aedfbda32be4c9618565482f6fad78ca4e8dda03e5b.lock
2024-01-18 09:27:35,855 - speechbrain.pretrained.fetching - INFO - HF fetch: /home/rajivratn/.cache/huggingface/hub/models--speechbrain--asr-transformer-transformerlm-librispeech/snapshots/4f5b13f894ba64cc36dc53e6c018a523e0a0a2f1/lm.ckpt
2024-01-18 09:27:35,856 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Delegating to Huggingface hub, source speechbrain/asr-transformer-transformerlm-librispeech.
2024-01-18 09:27:36,496 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /speechbrain/asr-transformer-transformerlm-librispeech/resolve/main/tokenizer.ckpt HTTP/1.1" 302 0
2024-01-18 09:27:36,498 - filelock - DEBUG - Attempting to acquire lock 140291350090320 on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/3cdc063492725aa2809a5fbb1aa790eda0e58370c810ebb54a8f4c8b2c46ea68.lock
2024-01-18 09:27:36,498 - filelock - DEBUG - Lock 140291350090320 acquired on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/3cdc063492725aa2809a5fbb1aa790eda0e58370c810ebb54a8f4c8b2c46ea68.lock
2024-01-18 09:27:37,367 - urllib3.connectionpool - DEBUG - https://cdn-lfs.huggingface.co:443 "GET /speechbrain/asr-transformer-transformerlm-librispeech/3cdc063492725aa2809a5fbb1aa790eda0e58370c810ebb54a8f4c8b2c46ea68?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.ckpt%3B+filename%3D%22tokenizer.ckpt%22%3B&Expires=1705829256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTgyOTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zcGVlY2hicmFpbi9hc3ItdHJhbnNmb3JtZXItdHJhbnNmb3JtZXJsbS1saWJyaXNwZWVjaC8zY2RjMDYzNDkyNzI1YWEyODA5YTVmYmIxYWE3OTBlZGEwZTU4MzcwYzgxMGViYjU0YThmNGM4YjJjNDZlYTY4P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=RKGdiA~2~RfluRsyAOcDbXrWjVSRO5I7kPXkRledCCEwtTrPAthBAVH1rFAxxWkEMuT3hvWQTcUYwGYsq-fbwbLFHRjiyJuTYk29tXxmz6gfbeMDhMsrbb8yXNnwiv-mGXwV5BszodfdsDprZP~RRbpR7YOwitdA-p90JGHjRBtwyTV5Xl9lS~2c7kR0yTseRb~xtxZ-CeCMLvXhUh12EtceIMb28xUwlXWQtbhT8kWPGG~WDjrSyTnNK50-4PfH~8RDPi0bxk6Q0jCzOlI3wfNhi2D4FY8~W96LdTFS7v791tvgBkTNyCL9hrWT2lKZuj3UVYY5PZO-aGjR56vQJA__&Key-Pair-Id=KVTP0A1DKRTAX HTTP/1.1" 200 324347
2024-01-18 09:27:37,371 - filelock - DEBUG - Attempting to release lock 140291350090320 on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/3cdc063492725aa2809a5fbb1aa790eda0e58370c810ebb54a8f4c8b2c46ea68.lock
2024-01-18 09:27:37,371 - filelock - DEBUG - Lock 140291350090320 released on /home/rajivratn/.cache/huggingface/hub/.locks/models--speechbrain--asr-transformer-transformerlm-librispeech/3cdc063492725aa2809a5fbb1aa790eda0e58370c810ebb54a8f4c8b2c46ea68.lock
2024-01-18 09:27:37,371 - speechbrain.pretrained.fetching - INFO - HF fetch: /home/rajivratn/.cache/huggingface/hub/models--speechbrain--asr-transformer-transformerlm-librispeech/snapshots/4f5b13f894ba64cc36dc53e6c018a523e0a0a2f1/tokenizer.ckpt
2024-01-18 09:27:37,372 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 09:27:37,957 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 09:27:37,957 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 09:27:37,957 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 09:27:38,086 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 09:27:38,103 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2024-01-18 09:27:38,103 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2024-01-18 09:27:42,017 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:27:42,029 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:27:42,140 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:27:42,145 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:28:00,852 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 528, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 283, in fit_batch
    (loss / self.grad_accumulation_factor).backward()
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
2024-01-18 09:28:44,314 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:28:44,314 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:28:44,757 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:28:46,408 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 09:28:47,349 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 09:28:47,350 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 09:28:47,350 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 09:28:47,351 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 09:28:47,695 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 09:28:47,695 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 09:28:47,695 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 09:28:47,831 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 09:28:47,849 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2024-01-18 09:28:47,849 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2024-01-18 09:28:51,267 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:28:51,279 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:28:51,389 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:28:51,394 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:30:36,094 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 285, in fit_batch
    if self.check_gradients(loss):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1131, in check_gradients
    torch.nn.utils.clip_grad_norm_(
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 12, in clip_grad_norm_
    def clip_grad_norm_(
    
KeyboardInterrupt
2024-01-18 09:32:56,722 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:32:56,723 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:32:57,123 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:32:58,921 - librispeech_prepare - INFO - Data_preparation...
2024-01-18 09:32:59,475 - librispeech_prepare - INFO - Csv file results/transformer/74443/train-clean-100.csv already exists, not recreating.
2024-01-18 09:33:01,993 - librispeech_prepare - INFO - Csv file results/transformer/74443/train-clean-360.csv already exists, not recreating.
2024-01-18 09:33:04,684 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/train-other-500.csv...
2024-01-18 09:33:11,025 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 452, in <module>
    run_on_main(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/distributed.py", line 60, in run_on_main
    func(*args, **kwargs)
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 137, in prepare_librispeech
    create_csv(
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/librispeech_prepare.py", line 334, in create_csv
    for row in parallel_map(line_processor, wav_lst, chunk_size=8192):
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/parallel.py", line 268, in parallel_map
    yield from mapper.run()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/parallel.py", line 92, in run
    yield from self._map_all()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/parallel.py", line 177, in _map_all
    raise self.remote_exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
2024-01-18 09:33:39,613 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:33:39,614 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:33:40,009 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:33:41,749 - librispeech_prepare - INFO - Data_preparation...
2024-01-18 09:33:42,116 - librispeech_prepare - INFO - Csv file results/transformer/74443/train-clean-100.csv already exists, not recreating.
2024-01-18 09:33:43,639 - librispeech_prepare - INFO - Csv file results/transformer/74443/train-clean-360.csv already exists, not recreating.
2024-01-18 09:33:45,953 - librispeech_prepare - INFO - Creating csv lists in  results/transformer/74443/train-other-500.csv...
2024-01-18 09:33:59,236 - librispeech_prepare - INFO - results/transformer/74443/train-other-500.csv successfully created!
2024-01-18 09:33:59,320 - librispeech_prepare - INFO - Csv file results/transformer/74443/dev-clean.csv already exists, not recreating.
2024-01-18 09:33:59,356 - librispeech_prepare - INFO - Csv file results/transformer/74443/test-clean.csv already exists, not recreating.
2024-01-18 09:33:59,394 - librispeech_prepare - INFO - Csv file results/transformer/74443/test-other.csv already exists, not recreating.
2024-01-18 09:33:59,395 - speechbrain.dataio.dataio - INFO - Skipping merging. Completed in previous run.
2024-01-18 09:33:59,761 - speechbrain.dataio.dataio - INFO - results/transformer/74443/train.csv is created.
2024-01-18 09:34:01,548 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 09:34:01,548 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 09:34:01,549 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 09:34:01,550 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 09:34:02,012 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 09:34:02,013 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 09:34:02,013 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 09:34:02,167 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 09:34:02,187 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2024-01-18 09:34:02,187 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2024-01-18 09:34:05,882 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:34:05,901 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:34:06,030 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:34:06,037 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:36:24,987 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 292, in fit_batch
    return loss.detach().cpu()
           ^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-18 09:37:49,754 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:37:49,755 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:37:50,146 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:37:51,956 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 09:37:53,712 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 09:37:53,712 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 09:37:53,713 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 09:37:53,837 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 09:37:54,262 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 09:37:54,262 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 09:37:54,262 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 09:37:54,352 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 09:37:54,435 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2024-01-18 09:37:54,435 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2024-01-18 09:37:57,907 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:37:57,919 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:37:58,027 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:37:58,032 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:41:54,287 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 292, in fit_batch
    return loss.detach().cpu()
           ^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-18 09:42:32,395 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 09:42:32,396 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 09:42:32,801 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 09:42:34,509 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 09:42:36,269 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 09:42:36,270 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 09:42:36,270 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 09:42:36,381 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 09:42:36,743 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 09:42:36,744 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 09:42:36,744 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 09:42:36,838 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 09:42:36,898 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2024-01-18 09:42:36,898 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2024-01-18 09:42:40,510 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:42:40,523 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:42:40,631 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 09:42:40,636 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 10:12:38,365 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-12-37+00
2024-01-18 10:14:56,935 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 4.39e-05, steps: 1098, optimizer: Adam - train loss: 2.91e+02 - valid loss: 1.51e+02, valid ACC: 1.74e-01
2024-01-18 10:14:57,886 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-14-56+00
2024-01-18 10:14:57,892 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2024-01-18 10:15:35,461 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 292, in fit_batch
    return loss.detach().cpu()
           ^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-18 10:15:55,815 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 10:15:55,815 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 10:15:56,323 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 10:15:59,600 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 10:16:01,461 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 10:16:01,461 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 10:16:01,461 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 10:16:01,462 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 10:16:01,855 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 10:16:01,855 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 10:16:01,855 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 10:16:01,971 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 10:16:02,419 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-18+10-14-56+00
2024-01-18 10:16:03,449 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2024-01-18 10:16:13,532 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 10:16:13,548 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 10:16:13,659 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 10:16:13,666 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 10:46:05,336 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-46-04+00
2024-01-18 10:46:05,480 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-12-37+00
2024-01-18 11:15:56,864 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 1.32e-04, steps: 3295, optimizer: Adam - train loss: 2.25e+02 - valid loss: 1.13e+02, valid ACC: 2.93e-01
2024-01-18 11:15:58,006 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+11-15-56+00
2024-01-18 11:15:58,015 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2024-01-18 11:45:59,722 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+11-45-58+00
2024-01-18 11:45:59,802 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-46-04+00
2024-01-18 12:16:26,609 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 2.20e-04, steps: 5492, optimizer: Adam - train loss: 1.53e+02 - valid loss: 41.14, valid ACC: 7.49e-01
2024-01-18 12:16:27,891 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+12-16-26+00
2024-01-18 12:16:27,913 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2024-01-18 12:46:29,336 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+12-46-28+00
2024-01-18 12:46:29,378 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+11-45-58+00
2024-01-18 13:17:06,518 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 3.08e-04, steps: 7689, optimizer: Adam - train loss: 62.89 - valid loss: 19.59, valid ACC: 8.83e-01
2024-01-18 13:17:07,853 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+13-17-06+00
2024-01-18 13:17:07,883 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2024-01-18 13:47:09,642 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+13-47-08+00
2024-01-18 13:47:09,768 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+12-46-28+00
2024-01-18 14:18:20,987 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 3.95e-04, steps: 9886, optimizer: Adam - train loss: 40.93 - valid loss: 15.01, valid ACC: 9.11e-01
2024-01-18 14:18:22,307 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+14-18-20+00
2024-01-18 14:18:22,353 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2024-01-18 19:54:56,969 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 19:54:56,970 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 19:54:57,591 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 19:55:00,765 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 19:55:02,750 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 19:55:02,750 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 19:55:02,750 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 19:55:02,751 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 19:55:03,814 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 19:55:03,814 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 19:55:03,814 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 19:55:03,976 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 19:55:03,997 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-18+14-18-20+00
2024-01-18 19:55:04,917 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2024-01-18 19:55:09,247 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:09,261 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:09,379 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:09,386 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:16,184 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 281, in fit_batch
    loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 135, in compute_objectives
    loss_ctc = self.hparams.ctc_cost(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/nnet/losses.py", line 280, in ctc_loss
    loss = torch.nn.functional.ctc_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/functional.py", line 2631, in ctc_loss
    return torch.ctc_loss(
           ^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-18 19:55:41,661 - speechbrain.core - INFO - Beginning experiment!
2024-01-18 19:55:41,662 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-18 19:55:42,059 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-18 19:55:43,850 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-18 19:55:45,655 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-18 19:55:45,656 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-18 19:55:45,657 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-18 19:55:45,658 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-18 19:55:46,074 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-18 19:55:46,075 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-18 19:55:46,075 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-18 19:55:46,160 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-18 19:55:46,183 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-18+14-18-20+00
2024-01-18 19:55:47,627 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2024-01-18 19:55:51,271 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:51,291 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:51,410 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 19:55:51,416 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-18 20:25:49,448 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+20-25-48+00
2024-01-18 20:25:49,505 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+13-47-08+00
2024-01-18 20:54:34,351 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 4.83e-04, steps: 12083, optimizer: Adam - train loss: 33.37 - valid loss: 12.59, valid ACC: 9.25e-01
2024-01-18 20:54:35,723 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+20-54-34+00
2024-01-18 20:54:35,743 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2024-01-18 21:24:37,248 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+21-24-36+00
2024-01-18 21:24:37,375 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+20-25-48+00
2024-01-18 21:53:15,650 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 5.71e-04, steps: 14280, optimizer: Adam - train loss: 28.93 - valid loss: 11.23, valid ACC: 9.33e-01
2024-01-18 21:53:16,915 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+21-53-15+00
2024-01-18 21:53:16,949 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2024-01-18 22:23:18,532 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+22-23-17+00
2024-01-18 22:23:18,711 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+21-24-36+00
2024-01-18 22:51:35,574 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 6.59e-04, steps: 16477, optimizer: Adam - train loss: 25.80 - valid loss: 9.81, valid ACC: 9.39e-01
2024-01-18 22:51:36,886 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+22-51-35+00
2024-01-18 22:51:36,940 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2024-01-18 23:21:38,185 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+23-21-37+00
2024-01-18 23:21:38,348 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+22-23-17+00
2024-01-18 23:50:07,143 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 7.47e-04, steps: 18674, optimizer: Adam - train loss: 23.55 - valid loss: 9.58, valid ACC: 9.40e-01
2024-01-18 23:50:08,493 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-18+23-50-07+00
2024-01-18 23:50:08,569 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2024-01-19 00:20:10,063 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+00-20-08+00
2024-01-19 00:20:10,265 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+23-21-37+00
2024-01-19 01:49:12,739 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 8.35e-04, steps: 20871, optimizer: Adam - train loss: 21.89 - valid loss: 8.66, valid ACC: 9.46e-01, valid WER: 7.07
2024-01-19 01:49:14,038 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+01-49-12+00
2024-01-19 01:49:14,140 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2024-01-19 02:19:15,351 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+02-19-14+00
2024-01-19 02:19:15,597 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+00-20-08+00
2024-01-19 02:47:49,784 - speechbrain.utils.train_logger - INFO - epoch: 11, lr: 9.23e-04, steps: 23068, optimizer: Adam - train loss: 20.49 - valid loss: 8.07, valid ACC: 9.48e-01
2024-01-19 02:47:51,154 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+02-47-49+00
2024-01-19 02:47:51,287 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+10-14-56+00
2024-01-19 02:47:51,287 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2024-01-19 03:17:52,720 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+03-17-51+00
2024-01-19 03:17:52,976 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+02-19-14+00
2024-01-19 03:46:26,089 - speechbrain.utils.train_logger - INFO - epoch: 12, lr: 9.95e-04, steps: 25265, optimizer: Adam - train loss: 19.38 - valid loss: 7.84, valid ACC: 9.50e-01
2024-01-19 03:46:27,418 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+03-46-26+00
2024-01-19 03:46:27,570 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+11-15-56+00
2024-01-19 03:46:27,571 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2024-01-19 03:56:33,894 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 527, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1366, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1193, in _fit_train
    loss = self.fit_batch(batch)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 281, in fit_batch
    loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 135, in compute_objectives
    loss_ctc = self.hparams.ctc_cost(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/nnet/losses.py", line 280, in ctc_loss
    loss = torch.nn.functional.ctc_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/functional.py", line 2631, in ctc_loss
    return torch.ctc_loss(
           ^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-19 03:56:58,448 - speechbrain.core - INFO - Beginning experiment!
2024-01-19 03:56:58,449 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-19 03:56:58,848 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-19 03:57:00,607 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 445, in <module>
    sb.create_experiment_directory(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 125, in create_experiment_directory
    sb.utils.distributed.ddp_barrier()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/distributed.py", line 118, in ddp_barrier
    torch.distributed.barrier()
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 3328, in barrier
    work = default_pg.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
2024-01-19 03:57:20,245 - speechbrain.core - INFO - Beginning experiment!
2024-01-19 03:57:20,245 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-19 03:57:20,632 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-19 03:57:22,499 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-19 03:57:24,403 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-19 03:57:24,403 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-19 03:57:24,404 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-19 03:57:24,418 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-19 03:57:24,835 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-19 03:57:24,835 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-19 03:57:24,835 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-19 03:57:24,983 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-19 03:57:25,045 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-19+03-46-26+00
2024-01-19 03:57:26,022 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2024-01-19 03:57:29,485 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-19 03:57:29,500 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-19 03:57:29,616 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-19 03:57:29,623 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.
2024-01-19 04:27:27,283 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+04-27-26+00
2024-01-19 04:27:27,328 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+03-17-51+00
2024-01-19 04:31:30,920 - speechbrain.utils.train_logger - INFO - epoch: 13, lr: 9.54e-04, steps: 27462, optimizer: Adam - train loss: 17.91 - valid loss: 7.29, valid ACC: 9.54e-01
2024-01-19 04:31:31,898 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+04-31-30+00
2024-01-19 04:31:31,949 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+12-16-26+00
2024-01-19 04:31:31,949 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2024-01-19 05:01:33,377 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-01-32+00
2024-01-19 05:01:33,546 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+04-27-26+00
2024-01-19 05:05:27,635 - speechbrain.utils.train_logger - INFO - epoch: 14, lr: 9.18e-04, steps: 29659, optimizer: Adam - train loss: 16.37 - valid loss: 6.79, valid ACC: 9.56e-01
2024-01-19 05:05:28,616 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-05-27+00
2024-01-19 05:05:28,676 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+13-17-06+00
2024-01-19 05:05:28,676 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2024-01-19 05:35:30,059 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-35-29+00
2024-01-19 05:35:30,241 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-01-32+00
2024-01-19 05:39:31,363 - speechbrain.utils.train_logger - INFO - epoch: 15, lr: 8.86e-04, steps: 31856, optimizer: Adam - train loss: 15.03 - valid loss: 6.57, valid ACC: 9.58e-01
2024-01-19 05:39:32,333 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-39-31+00
2024-01-19 05:39:32,436 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+14-18-20+00
2024-01-19 05:39:32,437 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2024-01-19 06:09:33,660 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-09-32+00
2024-01-19 06:09:33,845 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-35-29+00
2024-01-19 06:10:50,509 - speechbrain.utils.train_logger - INFO - epoch: 16, lr: 8.57e-04, steps: 34053, optimizer: Adam - train loss: 14.07 - valid loss: 6.29, valid ACC: 9.60e-01
2024-01-19 06:10:51,510 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-10-50+00
2024-01-19 06:10:51,604 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+20-54-34+00
2024-01-19 06:10:51,604 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2024-01-19 06:41:42,807 - speechbrain.utils.train_logger - INFO - epoch: 17, lr: 8.30e-04, steps: 36250, optimizer: Adam - train loss: 13.15 - valid loss: 6.17, valid ACC: 9.61e-01
2024-01-19 06:41:43,819 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-41-42+00
2024-01-19 06:41:43,922 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+21-53-15+00
2024-01-19 06:41:43,922 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2024-01-19 07:12:29,375 - speechbrain.utils.train_logger - INFO - epoch: 18, lr: 8.06e-04, steps: 38447, optimizer: Adam - train loss: 12.45 - valid loss: 5.84, valid ACC: 9.62e-01
2024-01-19 07:12:30,355 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+07-12-29+00
2024-01-19 07:12:30,474 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+22-51-35+00
2024-01-19 07:12:30,475 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2024-01-19 07:43:25,886 - speechbrain.utils.train_logger - INFO - epoch: 19, lr: 7.84e-04, steps: 40644, optimizer: Adam - train loss: 11.85 - valid loss: 5.79, valid ACC: 9.63e-01
2024-01-19 07:43:26,860 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+07-43-25+00
2024-01-19 07:43:26,983 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-18+23-50-07+00
2024-01-19 07:43:26,983 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2024-01-19 08:13:28,371 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+08-13-27+00
2024-01-19 08:13:28,500 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-09-32+00
2024-01-19 09:18:57,789 - speechbrain.utils.train_logger - INFO - epoch: 20, lr: 7.64e-04, steps: 42841, optimizer: Adam - train loss: 11.26 - valid loss: 5.77, valid ACC: 9.63e-01, valid WER: 4.53
2024-01-19 09:18:58,798 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-18-57+00
2024-01-19 09:18:58,940 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+01-49-12+00
2024-01-19 09:18:58,941 - speechbrain.utils.epoch_loop - INFO - Going into epoch 21
2024-01-19 09:49:00,293 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-48-59+00
2024-01-19 09:49:00,429 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+08-13-27+00
2024-01-19 09:50:28,575 - speechbrain.utils.train_logger - INFO - epoch: 21, lr: 7.45e-04, steps: 45038, optimizer: Adam - train loss: 10.74 - valid loss: 5.85, valid ACC: 9.63e-01
2024-01-19 09:50:29,551 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-50-28+00
2024-01-19 09:50:29,711 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+02-47-49+00
2024-01-19 09:50:29,712 - speechbrain.utils.epoch_loop - INFO - Going into epoch 22
2024-01-19 10:21:44,135 - speechbrain.utils.train_logger - INFO - epoch: 22, lr: 7.28e-04, steps: 47235, optimizer: Adam - train loss: 10.34 - valid loss: 5.67, valid ACC: 9.64e-01
2024-01-19 10:21:45,083 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+10-21-44+00
2024-01-19 10:21:45,258 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+03-46-26+00
2024-01-19 10:21:45,259 - speechbrain.utils.epoch_loop - INFO - Going into epoch 23
2024-01-19 10:52:48,456 - speechbrain.utils.train_logger - INFO - epoch: 23, lr: 7.11e-04, steps: 49432, optimizer: Adam - train loss: 9.96 - valid loss: 5.54, valid ACC: 9.65e-01
2024-01-19 10:52:49,565 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+10-52-48+00
2024-01-19 10:52:49,762 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+04-31-30+00
2024-01-19 10:52:49,762 - speechbrain.utils.epoch_loop - INFO - Going into epoch 24
2024-01-19 11:23:47,919 - speechbrain.utils.train_logger - INFO - epoch: 24, lr: 6.96e-04, steps: 51629, optimizer: Adam - train loss: 9.59 - valid loss: 5.47, valid ACC: 9.65e-01
2024-01-19 11:23:48,919 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-23-47+00
2024-01-19 11:23:49,126 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-05-27+00
2024-01-19 11:23:49,127 - speechbrain.utils.epoch_loop - INFO - Going into epoch 25
2024-01-19 11:53:50,231 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-53-49+00
2024-01-19 11:53:50,431 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-48-59+00
2024-01-19 11:55:19,685 - speechbrain.utils.train_logger - INFO - epoch: 25, lr: 6.82e-04, steps: 53826, optimizer: Adam - train loss: 9.25 - valid loss: 5.45, valid ACC: 9.65e-01
2024-01-19 11:55:20,585 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-55-19+00
2024-01-19 11:55:20,795 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+05-39-31+00
2024-01-19 11:55:20,796 - speechbrain.utils.epoch_loop - INFO - Going into epoch 26
2024-01-19 12:26:27,669 - speechbrain.utils.train_logger - INFO - epoch: 26, lr: 6.68e-04, steps: 56023, optimizer: Adam - train loss: 8.93 - valid loss: 5.41, valid ACC: 9.66e-01
2024-01-19 12:26:28,648 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+12-26-27+00
2024-01-19 12:26:28,866 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-10-50+00
2024-01-19 12:26:28,867 - speechbrain.utils.epoch_loop - INFO - Going into epoch 27
2024-01-19 12:57:33,186 - speechbrain.utils.train_logger - INFO - epoch: 27, lr: 6.55e-04, steps: 58220, optimizer: Adam - train loss: 8.69 - valid loss: 5.33, valid ACC: 9.67e-01
2024-01-19 12:57:34,187 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+12-57-33+00
2024-01-19 12:57:34,420 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+06-41-42+00
2024-01-19 12:57:34,420 - speechbrain.utils.epoch_loop - INFO - Going into epoch 28
2024-01-19 13:27:35,691 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-27-34+00
2024-01-19 13:27:35,924 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-53-49+00
2024-01-19 13:29:03,742 - speechbrain.utils.train_logger - INFO - epoch: 28, lr: 6.43e-04, steps: 60417, optimizer: Adam - train loss: 8.44 - valid loss: 5.34, valid ACC: 9.66e-01
2024-01-19 13:29:04,744 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-29-03+00
2024-01-19 13:29:05,052 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+07-12-29+00
2024-01-19 13:29:05,053 - speechbrain.utils.epoch_loop - INFO - Going into epoch 29
2024-01-19 13:59:54,039 - speechbrain.utils.train_logger - INFO - epoch: 29, lr: 6.32e-04, steps: 62614, optimizer: Adam - train loss: 8.23 - valid loss: 5.43, valid ACC: 9.66e-01
2024-01-19 13:59:55,010 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-59-54+00
2024-01-19 13:59:55,270 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+07-43-25+00
2024-01-19 13:59:55,271 - speechbrain.utils.epoch_loop - INFO - Going into epoch 30
2024-01-19 14:29:56,844 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+14-29-55+00
2024-01-19 14:29:57,111 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-27-34+00
2024-01-19 15:39:36,833 - speechbrain.utils.train_logger - INFO - epoch: 30, lr: 6.21e-04, steps: 64811, optimizer: Adam - train loss: 7.98 - valid loss: 5.33, valid ACC: 9.67e-01, valid WER: 3.95
2024-01-19 15:39:37,827 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+15-39-36+00
2024-01-19 15:39:38,109 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-50-28+00
2024-01-19 15:39:38,110 - speechbrain.utils.epoch_loop - INFO - Going into epoch 31
2024-01-19 16:10:33,052 - speechbrain.utils.train_logger - INFO - epoch: 31, lr: 6.11e-04, steps: 67008, optimizer: Adam - train loss: 7.76 - valid loss: 5.26, valid ACC: 9.68e-01
2024-01-19 16:10:34,014 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+16-10-33+00
2024-01-19 16:10:34,308 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+09-18-57+00
2024-01-19 16:10:34,308 - speechbrain.utils.epoch_loop - INFO - Going into epoch 32
2024-01-19 16:41:38,664 - speechbrain.utils.train_logger - INFO - epoch: 32, lr: 6.01e-04, steps: 69205, optimizer: Adam - train loss: 7.59 - valid loss: 5.18, valid ACC: 9.68e-01
2024-01-19 16:41:39,615 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+16-41-38+00
2024-01-19 16:41:39,917 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+10-21-44+00
2024-01-19 16:41:39,917 - speechbrain.utils.epoch_loop - INFO - Going into epoch 33
2024-01-19 17:11:41,358 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-11-40+00
2024-01-19 17:11:41,666 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+14-29-55+00
2024-01-19 17:12:58,446 - speechbrain.utils.train_logger - INFO - epoch: 33, lr: 5.92e-04, steps: 71402, optimizer: Adam - train loss: 7.44 - valid loss: 5.40, valid ACC: 9.67e-01
2024-01-19 17:12:59,431 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-12-58+00
2024-01-19 17:12:59,759 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+10-52-48+00
2024-01-19 17:12:59,759 - speechbrain.utils.epoch_loop - INFO - Going into epoch 34
2024-01-19 17:43:01,126 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-43-00+00
2024-01-19 17:43:01,541 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-11-40+00
2024-01-19 17:44:15,754 - speechbrain.utils.train_logger - INFO - epoch: 34, lr: 5.83e-04, steps: 73599, optimizer: Adam - train loss: 7.28 - valid loss: 5.36, valid ACC: 9.67e-01
2024-01-19 17:44:16,741 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-44-15+00
2024-01-19 17:44:17,074 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-55-19+00
2024-01-19 17:44:17,074 - speechbrain.utils.epoch_loop - INFO - Going into epoch 35
2024-01-19 18:15:29,035 - speechbrain.utils.train_logger - INFO - epoch: 35, lr: 5.74e-04, steps: 75796, optimizer: Adam - train loss: 7.10 - valid loss: 5.34, valid ACC: 9.68e-01
2024-01-19 18:15:30,199 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+18-15-29+00
2024-01-19 18:15:30,546 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+11-23-47+00
2024-01-19 18:15:30,546 - speechbrain.utils.epoch_loop - INFO - Going into epoch 36
2024-01-19 18:46:40,340 - speechbrain.utils.train_logger - INFO - epoch: 36, lr: 5.66e-04, steps: 77993, optimizer: Adam - train loss: 6.96 - valid loss: 5.44, valid ACC: 9.67e-01
2024-01-19 18:46:41,311 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+18-46-40+00
2024-01-19 18:46:41,665 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+12-26-27+00
2024-01-19 18:46:41,665 - speechbrain.utils.epoch_loop - INFO - Going into epoch 37
2024-01-19 19:17:52,448 - speechbrain.utils.train_logger - INFO - epoch: 37, lr: 5.58e-04, steps: 80190, optimizer: Adam - train loss: 6.82 - valid loss: 5.28, valid ACC: 9.68e-01
2024-01-19 19:17:53,423 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+19-17-52+00
2024-01-19 19:17:53,784 - speechbrain.utils.epoch_loop - INFO - Going into epoch 38
2024-01-19 19:48:57,723 - speechbrain.utils.train_logger - INFO - epoch: 38, lr: 5.51e-04, steps: 82387, optimizer: Adam - train loss: 6.70 - valid loss: 5.41, valid ACC: 9.68e-01
2024-01-19 19:48:58,695 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+19-48-57+00
2024-01-19 19:48:59,104 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-29-03+00
2024-01-19 19:48:59,105 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+13-59-54+00
2024-01-19 19:48:59,106 - speechbrain.utils.epoch_loop - INFO - Going into epoch 39
2024-01-19 20:20:04,624 - speechbrain.utils.train_logger - INFO - epoch: 39, lr: 5.44e-04, steps: 84584, optimizer: Adam - train loss: 6.57 - valid loss: 5.24, valid ACC: 9.69e-01
2024-01-19 20:20:05,606 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+20-20-04+00
2024-01-19 20:20:05,994 - speechbrain.utils.epoch_loop - INFO - Going into epoch 40
2024-01-19 21:55:19,814 - speechbrain.utils.train_logger - INFO - epoch: 40, lr: 5.37e-04, steps: 86781, optimizer: Adam - train loss: 6.45 - valid loss: 5.33, valid ACC: 9.68e-01, valid WER: 3.73
2024-01-19 21:55:20,836 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+21-55-19+00
2024-01-19 21:55:21,266 - speechbrain.utils.epoch_loop - INFO - Going into epoch 41
2024-01-19 22:26:26,744 - speechbrain.utils.train_logger - INFO - epoch: 41, lr: 5.30e-04, steps: 88978, optimizer: Adam - train loss: 6.34 - valid loss: 5.22, valid ACC: 9.69e-01
2024-01-19 22:26:27,712 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-26-26+00
2024-01-19 22:26:28,177 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+12-57-33+00
2024-01-19 22:26:28,177 - speechbrain.utils.epoch_loop - INFO - Going into epoch 42
2024-01-19 22:56:29,320 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-56-28+00
2024-01-19 22:56:29,811 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-43-00+00
2024-01-19 22:57:44,172 - speechbrain.utils.train_logger - INFO - epoch: 42, lr: 5.24e-04, steps: 91175, optimizer: Adam - train loss: 6.25 - valid loss: 5.30, valid ACC: 9.68e-01
2024-01-19 22:57:45,114 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-57-44+00
2024-01-19 22:57:45,626 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+15-39-36+00
2024-01-19 22:57:45,639 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-12-58+00
2024-01-19 22:57:45,640 - speechbrain.utils.epoch_loop - INFO - Going into epoch 43
2024-01-19 23:27:47,079 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-27-45+00
2024-01-19 23:27:47,694 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-56-28+00
2024-01-19 23:29:03,322 - speechbrain.utils.train_logger - INFO - epoch: 43, lr: 5.17e-04, steps: 93372, optimizer: Adam - train loss: 6.18 - valid loss: 5.28, valid ACC: 9.69e-01
2024-01-19 23:29:04,268 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-29-03+00
2024-01-19 23:29:04,760 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+17-44-15+00
2024-01-19 23:29:04,760 - speechbrain.utils.epoch_loop - INFO - Going into epoch 44
2024-01-19 23:59:06,230 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-59-05+00
2024-01-19 23:59:06,841 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-27-45+00
2024-01-20 00:00:16,893 - speechbrain.utils.train_logger - INFO - epoch: 44, lr: 5.11e-04, steps: 95569, optimizer: Adam - train loss: 6.06 - valid loss: 5.30, valid ACC: 9.70e-01
2024-01-20 00:00:17,867 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+00-00-16+00
2024-01-20 00:00:18,385 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+18-15-29+00
2024-01-20 00:00:18,386 - speechbrain.utils.epoch_loop - INFO - Going into epoch 45
2024-01-20 00:31:24,032 - speechbrain.utils.train_logger - INFO - epoch: 45, lr: 5.06e-04, steps: 97766, optimizer: Adam - train loss: 5.96 - valid loss: 5.42, valid ACC: 9.69e-01
2024-01-20 00:31:24,994 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+00-31-24+00
2024-01-20 00:31:25,520 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+18-46-40+00
2024-01-20 00:31:25,521 - speechbrain.utils.epoch_loop - INFO - Going into epoch 46
2024-01-20 01:02:20,971 - speechbrain.utils.train_logger - INFO - epoch: 46, lr: 5.00e-04, steps: 99963, optimizer: Adam - train loss: 5.88 - valid loss: 5.49, valid ACC: 9.69e-01
2024-01-20 01:02:21,928 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+01-02-20+00
2024-01-20 01:02:22,467 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+19-17-52+00
2024-01-20 01:02:22,467 - speechbrain.utils.epoch_loop - INFO - Going into epoch 47
2024-01-20 01:33:28,004 - speechbrain.utils.train_logger - INFO - epoch: 47, lr: 4.95e-04, steps: 102160, optimizer: Adam - train loss: 5.82 - valid loss: 5.36, valid ACC: 9.68e-01
2024-01-20 01:33:28,967 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+01-33-28+00
2024-01-20 01:33:29,516 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+16-10-33+00
2024-01-20 01:33:29,534 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+19-48-57+00
2024-01-20 01:33:29,534 - speechbrain.utils.epoch_loop - INFO - Going into epoch 48
2024-01-20 02:03:30,917 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-03-29+00
2024-01-20 02:03:31,535 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-59-05+00
2024-01-20 02:04:39,154 - speechbrain.utils.train_logger - INFO - epoch: 48, lr: 4.89e-04, steps: 104357, optimizer: Adam - train loss: 5.71 - valid loss: 5.55, valid ACC: 9.69e-01
2024-01-20 02:04:40,263 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-04-39+00
2024-01-20 02:04:40,799 - speechbrain.utils.epoch_loop - INFO - Going into epoch 49
2024-01-20 02:34:42,375 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-34-41+00
2024-01-20 02:34:43,065 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-03-29+00
2024-01-20 02:35:57,657 - speechbrain.utils.train_logger - INFO - epoch: 49, lr: 4.84e-04, steps: 106554, optimizer: Adam - train loss: 5.68 - valid loss: 5.57, valid ACC: 9.68e-01
2024-01-20 02:35:58,636 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-35-57+00
2024-01-20 02:35:59,250 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+21-55-19+00
2024-01-20 02:35:59,250 - speechbrain.utils.epoch_loop - INFO - Going into epoch 50
2024-01-20 04:10:55,562 - speechbrain.utils.train_logger - INFO - epoch: 50, lr: 4.79e-04, steps: 108751, optimizer: Adam - train loss: 5.59 - valid loss: 5.38, valid ACC: 9.69e-01, valid WER: 3.64
2024-01-20 04:10:56,504 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+04-10-55+00
2024-01-20 04:10:57,088 - speechbrain.utils.epoch_loop - INFO - Going into epoch 51
2024-01-20 04:41:59,000 - speechbrain.utils.train_logger - INFO - epoch: 51, lr: 4.75e-04, steps: 110948, optimizer: Adam - train loss: 5.50 - valid loss: 5.34, valid ACC: 9.70e-01
2024-01-20 04:42:00,011 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+04-41-59+00
2024-01-20 04:42:00,682 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-57-44+00
2024-01-20 04:42:00,698 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+16-41-38+00
2024-01-20 04:42:00,698 - speechbrain.utils.epoch_loop - INFO - Going into epoch 52
2024-01-20 05:13:03,967 - speechbrain.utils.train_logger - INFO - epoch: 52, lr: 4.70e-04, steps: 113145, optimizer: Adam - train loss: 5.44 - valid loss: 5.47, valid ACC: 9.69e-01
2024-01-20 05:13:04,955 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+05-13-03+00
2024-01-20 05:13:05,564 - speechbrain.utils.epoch_loop - INFO - Going into epoch 53
2024-01-20 05:43:59,816 - speechbrain.utils.train_logger - INFO - epoch: 53, lr: 4.66e-04, steps: 115342, optimizer: Adam - train loss: 5.38 - valid loss: 5.50, valid ACC: 9.69e-01
2024-01-20 05:44:00,907 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+05-43-59+00
2024-01-20 05:44:01,570 - speechbrain.utils.epoch_loop - INFO - Going into epoch 54
2024-01-20 06:14:02,935 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-14-01+00
2024-01-20 06:14:03,768 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-34-41+00
2024-01-20 06:15:17,969 - speechbrain.utils.train_logger - INFO - epoch: 54, lr: 4.61e-04, steps: 117539, optimizer: Adam - train loss: 5.32 - valid loss: 5.39, valid ACC: 9.70e-01
2024-01-20 06:15:18,969 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-15-17+00
2024-01-20 06:15:19,723 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+20-20-04+00
2024-01-20 06:15:19,739 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+00-31-24+00
2024-01-20 06:15:19,740 - speechbrain.utils.epoch_loop - INFO - Going into epoch 55
2024-01-20 06:46:26,861 - speechbrain.utils.train_logger - INFO - epoch: 55, lr: 4.57e-04, steps: 119736, optimizer: Adam - train loss: 5.24 - valid loss: 5.61, valid ACC: 9.69e-01
2024-01-20 06:46:27,855 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-46-26+00
2024-01-20 06:46:28,561 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+23-29-03+00
2024-01-20 06:46:28,561 - speechbrain.utils.epoch_loop - INFO - Going into epoch 56
2024-01-20 07:17:29,806 - speechbrain.utils.train_logger - INFO - epoch: 56, lr: 4.53e-04, steps: 121933, optimizer: Adam - train loss: 5.19 - valid loss: 5.56, valid ACC: 9.69e-01
2024-01-20 07:17:30,755 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+07-17-29+00
2024-01-20 07:17:31,496 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+01-33-28+00
2024-01-20 07:17:31,496 - speechbrain.utils.epoch_loop - INFO - Going into epoch 57
2024-01-20 07:48:33,786 - speechbrain.utils.train_logger - INFO - epoch: 57, lr: 4.49e-04, steps: 124130, optimizer: Adam - train loss: 5.15 - valid loss: 5.30, valid ACC: 9.70e-01
2024-01-20 07:48:34,789 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+07-48-33+00
2024-01-20 07:48:35,514 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-04-39+00
2024-01-20 07:48:35,530 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+01-02-20+00
2024-01-20 07:48:35,530 - speechbrain.utils.epoch_loop - INFO - Going into epoch 58
2024-01-20 08:19:34,289 - speechbrain.utils.train_logger - INFO - epoch: 58, lr: 4.45e-04, steps: 126327, optimizer: Adam - train loss: 5.09 - valid loss: 5.43, valid ACC: 9.70e-01
2024-01-20 08:19:35,293 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+08-19-34+00
2024-01-20 08:19:35,994 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+02-35-57+00
2024-01-20 08:19:35,996 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-19+22-26-26+00
2024-01-20 08:19:35,997 - speechbrain.utils.epoch_loop - INFO - Going into epoch 59
2024-01-20 08:50:24,628 - speechbrain.utils.train_logger - INFO - epoch: 59, lr: 4.41e-04, steps: 128524, optimizer: Adam - train loss: 5.05 - valid loss: 5.58, valid ACC: 9.69e-01
2024-01-20 08:50:25,590 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+08-50-24+00
2024-01-20 08:50:26,232 - speechbrain.utils.epoch_loop - INFO - Going into epoch 60
2024-01-20 10:25:28,709 - speechbrain.utils.train_logger - INFO - epoch: 60, lr: 4.37e-04, steps: 130721, optimizer: Adam - train loss: 4.98 - valid loss: 5.49, valid ACC: 9.70e-01, valid WER: 3.54
2024-01-20 10:25:29,647 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+10-25-28+00
2024-01-20 10:25:30,350 - speechbrain.utils.epoch_loop - INFO - Going into epoch 61
2024-01-20 10:56:24,095 - speechbrain.utils.train_logger - INFO - epoch: 61, lr: 4.34e-04, steps: 132918, optimizer: Adam - train loss: 4.92 - valid loss: 5.50, valid ACC: 9.71e-01
2024-01-20 10:56:25,048 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+10-56-24+00
2024-01-20 10:56:25,810 - speechbrain.utils.epoch_loop - INFO - Going into epoch 62
2024-01-20 11:27:22,818 - speechbrain.utils.train_logger - INFO - epoch: 62, lr: 4.30e-04, steps: 135115, optimizer: Adam - train loss: 4.88 - valid loss: 5.53, valid ACC: 9.70e-01
2024-01-20 11:27:23,786 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+11-27-22+00
2024-01-20 11:27:24,614 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+05-43-59+00
2024-01-20 11:27:24,614 - speechbrain.utils.epoch_loop - INFO - Going into epoch 63
2024-01-20 11:58:21,559 - speechbrain.utils.train_logger - INFO - epoch: 63, lr: 4.27e-04, steps: 137312, optimizer: Adam - train loss: 4.85 - valid loss: 5.49, valid ACC: 9.71e-01
2024-01-20 11:58:22,530 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+11-58-21+00
2024-01-20 11:58:23,396 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+04-10-55+00
2024-01-20 11:58:23,415 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-14-01+00
2024-01-20 11:58:23,416 - speechbrain.utils.epoch_loop - INFO - Going into epoch 64
2024-01-20 12:29:27,840 - speechbrain.utils.train_logger - INFO - epoch: 64, lr: 4.23e-04, steps: 139509, optimizer: Adam - train loss: 4.79 - valid loss: 5.57, valid ACC: 9.70e-01
2024-01-20 12:29:28,839 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+12-29-27+00
2024-01-20 12:29:29,667 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+05-13-03+00
2024-01-20 12:29:29,668 - speechbrain.utils.epoch_loop - INFO - Going into epoch 65
2024-01-20 13:00:31,170 - speechbrain.utils.train_logger - INFO - epoch: 65, lr: 4.20e-04, steps: 141706, optimizer: Adam - train loss: 4.76 - valid loss: 5.47, valid ACC: 9.70e-01
2024-01-20 13:00:32,165 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+13-00-31+00
2024-01-20 13:00:32,990 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-46-26+00
2024-01-20 13:00:32,995 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+06-15-17+00
2024-01-20 13:00:32,995 - speechbrain.utils.epoch_loop - INFO - Going into epoch 66
2024-01-20 13:31:28,200 - speechbrain.utils.train_logger - INFO - epoch: 66, lr: 4.17e-04, steps: 143903, optimizer: Adam - train loss: 4.72 - valid loss: 5.55, valid ACC: 9.70e-01
2024-01-20 13:31:29,173 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+13-31-28+00
2024-01-20 13:31:29,949 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+00-00-16+00
2024-01-20 13:31:29,950 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+07-17-29+00
2024-01-20 13:31:29,950 - speechbrain.utils.epoch_loop - INFO - Going into epoch 67
2024-01-20 14:02:40,068 - speechbrain.utils.train_logger - INFO - epoch: 67, lr: 4.14e-04, steps: 146100, optimizer: Adam - train loss: 4.68 - valid loss: 5.46, valid ACC: 9.70e-01
2024-01-20 14:02:41,070 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+14-02-40+00
2024-01-20 14:02:41,815 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+04-41-59+00
2024-01-20 14:02:41,816 - speechbrain.utils.epoch_loop - INFO - Going into epoch 68
2024-01-20 14:33:35,818 - speechbrain.utils.train_logger - INFO - epoch: 68, lr: 4.11e-04, steps: 148297, optimizer: Adam - train loss: 4.63 - valid loss: 5.56, valid ACC: 9.71e-01
2024-01-20 14:33:36,803 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+14-33-35+00
2024-01-20 14:33:37,541 - speechbrain.utils.epoch_loop - INFO - Going into epoch 69
2024-01-20 15:04:40,149 - speechbrain.utils.train_logger - INFO - epoch: 69, lr: 4.08e-04, steps: 150494, optimizer: Adam - train loss: 4.59 - valid loss: 5.54, valid ACC: 9.71e-01
2024-01-20 15:04:41,138 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+15-04-40+00
2024-01-20 15:04:41,946 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+08-50-24+00
2024-01-20 15:04:41,946 - speechbrain.utils.epoch_loop - INFO - Going into epoch 70
2024-01-20 16:40:04,032 - speechbrain.utils.train_logger - INFO - epoch: 70, lr: 4.05e-04, steps: 152691, optimizer: Adam - train loss: 4.56 - valid loss: 5.60, valid ACC: 9.70e-01, valid WER: 3.47
2024-01-20 16:40:04,962 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+16-40-04+00
2024-01-20 16:40:05,786 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+10-25-28+00
2024-01-20 16:40:05,786 - speechbrain.utils.epoch_loop - INFO - Going into epoch 71
2024-01-20 17:11:13,750 - speechbrain.utils.train_logger - INFO - epoch: 71, lr: 4.02e-04, steps: 154888, optimizer: Adam - train loss: 4.52 - valid loss: 5.57, valid ACC: 9.70e-01
2024-01-20 17:11:14,772 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+17-11-13+00
2024-01-20 17:11:15,605 - speechbrain.utils.epoch_loop - INFO - Going into epoch 72
2024-01-20 17:42:18,437 - speechbrain.utils.train_logger - INFO - epoch: 72, lr: 3.99e-04, steps: 157085, optimizer: Adam - train loss: 4.47 - valid loss: 5.53, valid ACC: 9.70e-01
2024-01-20 17:42:19,443 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+17-42-18+00
2024-01-20 17:42:20,347 - speechbrain.utils.epoch_loop - INFO - Going into epoch 73
2024-01-20 18:13:23,056 - speechbrain.utils.train_logger - INFO - epoch: 73, lr: 3.96e-04, steps: 159282, optimizer: Adam - train loss: 4.46 - valid loss: 5.58, valid ACC: 9.71e-01
2024-01-20 18:13:24,049 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+18-13-23+00
2024-01-20 18:13:25,032 - speechbrain.utils.epoch_loop - INFO - Going into epoch 74
2024-01-20 18:44:25,056 - speechbrain.utils.train_logger - INFO - epoch: 74, lr: 3.93e-04, steps: 161479, optimizer: Adam - train loss: 4.40 - valid loss: 5.68, valid ACC: 9.70e-01
2024-01-20 18:44:26,000 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+18-44-25+00
2024-01-20 18:44:27,054 - speechbrain.utils.epoch_loop - INFO - Going into epoch 75
2024-01-20 19:14:28,440 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-14-27+00
2024-01-20 19:15:35,078 - speechbrain.utils.train_logger - INFO - epoch: 75, lr: 3.91e-04, steps: 163676, optimizer: Adam - train loss: 4.38 - valid loss: 5.51, valid ACC: 9.70e-01
2024-01-20 19:15:36,037 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-15-35+00
2024-01-20 19:15:37,236 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+11-27-22+00
2024-01-20 19:15:37,252 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+13-31-28+00
2024-01-20 19:15:37,257 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+13-00-31+00
2024-01-20 19:15:37,257 - speechbrain.utils.epoch_loop - INFO - Going into epoch 76
2024-01-20 19:46:42,088 - speechbrain.utils.train_logger - INFO - epoch: 76, lr: 3.88e-04, steps: 165873, optimizer: Adam - train loss: 4.31 - valid loss: 5.53, valid ACC: 9.71e-01
2024-01-20 19:46:43,089 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-46-42+00
2024-01-20 19:46:44,180 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+14-02-40+00
2024-01-20 19:46:44,180 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+12-29-27+00
2024-01-20 19:46:44,181 - speechbrain.utils.epoch_loop - INFO - Going into epoch 77
2024-01-20 20:17:48,457 - speechbrain.utils.train_logger - INFO - epoch: 77, lr: 3.86e-04, steps: 168070, optimizer: Adam - train loss: 4.30 - valid loss: 5.62, valid ACC: 9.70e-01
2024-01-20 20:17:49,477 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-17-48+00
2024-01-20 20:17:50,527 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+07-48-33+00
2024-01-20 20:17:50,527 - speechbrain.utils.epoch_loop - INFO - Going into epoch 78
2024-01-20 20:47:51,788 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-47-50+00
2024-01-20 20:47:52,798 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-14-27+00
2024-01-20 20:49:05,816 - speechbrain.utils.train_logger - INFO - epoch: 78, lr: 3.83e-04, steps: 170267, optimizer: Adam - train loss: 4.29 - valid loss: 5.51, valid ACC: 9.70e-01
2024-01-20 20:49:06,803 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-49-05+00
2024-01-20 20:49:07,869 - speechbrain.utils.epoch_loop - INFO - Going into epoch 79
2024-01-20 21:19:08,977 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-19-08+00
2024-01-20 21:19:10,181 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-47-50+00
2024-01-20 21:20:32,500 - speechbrain.utils.train_logger - INFO - epoch: 79, lr: 3.81e-04, steps: 172464, optimizer: Adam - train loss: 4.23 - valid loss: 5.75, valid ACC: 9.70e-01
2024-01-20 21:20:33,506 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-20-32+00
2024-01-20 21:20:34,672 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+16-40-04+00
2024-01-20 21:20:34,673 - speechbrain.utils.epoch_loop - INFO - Going into epoch 80
2024-01-20 21:50:36,159 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-50-35+00
2024-01-20 21:50:37,368 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-19-08+00
2024-01-20 22:55:53,817 - speechbrain.utils.train_logger - INFO - epoch: 80, lr: 3.78e-04, steps: 174661, optimizer: Adam - train loss: 4.23 - valid loss: 5.57, valid ACC: 9.71e-01, valid WER: 3.43
2024-01-20 22:55:54,831 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+22-55-53+00
2024-01-20 22:55:56,003 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+17-11-13+00
2024-01-20 22:55:56,008 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+08-19-34+00
2024-01-20 22:55:56,008 - speechbrain.utils.epoch_loop - INFO - Going into epoch 81
2024-01-20 23:25:57,423 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-25-56+00
2024-01-20 23:25:58,629 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-50-35+00
2024-01-20 23:27:36,261 - speechbrain.utils.train_logger - INFO - epoch: 81, lr: 3.76e-04, steps: 176858, optimizer: Adam - train loss: 4.19 - valid loss: 5.74, valid ACC: 9.70e-01
2024-01-20 23:27:37,260 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-27-36+00
2024-01-20 23:27:38,405 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+17-42-18+00
2024-01-20 23:27:38,406 - speechbrain.utils.epoch_loop - INFO - Going into epoch 82
2024-01-20 23:57:39,787 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-57-38+00
2024-01-20 23:57:41,010 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-25-56+00
2024-01-20 23:59:21,038 - speechbrain.utils.train_logger - INFO - epoch: 82, lr: 3.74e-04, steps: 179055, optimizer: Adam - train loss: 4.17 - valid loss: 5.64, valid ACC: 9.71e-01
2024-01-20 23:59:22,028 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-59-21+00
2024-01-20 23:59:23,190 - speechbrain.utils.epoch_loop - INFO - Going into epoch 83
2024-01-21 00:29:24,456 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+00-29-23+00
2024-01-21 00:29:25,816 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-57-38+00
2024-01-21 00:31:07,534 - speechbrain.utils.train_logger - INFO - epoch: 83, lr: 3.71e-04, steps: 181252, optimizer: Adam - train loss: 4.12 - valid loss: 5.72, valid ACC: 9.71e-01
2024-01-21 00:31:08,466 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+00-31-07+00
2024-01-21 00:31:09,726 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+18-44-25+00
2024-01-21 00:31:09,726 - speechbrain.utils.epoch_loop - INFO - Going into epoch 84
2024-01-21 01:01:10,978 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-01-09+00
2024-01-21 01:01:12,331 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+00-29-23+00
2024-01-21 01:02:46,123 - speechbrain.utils.train_logger - INFO - epoch: 84, lr: 3.69e-04, steps: 183449, optimizer: Adam - train loss: 4.10 - valid loss: 5.67, valid ACC: 9.71e-01
2024-01-21 01:02:47,130 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-02-46+00
2024-01-21 01:02:48,416 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-15-35+00
2024-01-21 01:02:48,417 - speechbrain.utils.epoch_loop - INFO - Going into epoch 85
2024-01-21 01:32:50,072 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-32-48+00
2024-01-21 01:32:51,459 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-01-09+00
2024-01-21 01:34:14,419 - speechbrain.utils.train_logger - INFO - epoch: 85, lr: 3.67e-04, steps: 185646, optimizer: Adam - train loss: 4.09 - valid loss: 5.61, valid ACC: 9.70e-01
2024-01-21 01:34:15,434 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-34-14+00
2024-01-21 01:34:16,734 - speechbrain.utils.epoch_loop - INFO - Going into epoch 86
2024-01-21 02:04:17,929 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-04-16+00
2024-01-21 02:04:19,406 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-32-48+00
2024-01-21 02:05:37,072 - speechbrain.utils.train_logger - INFO - epoch: 86, lr: 3.65e-04, steps: 187843, optimizer: Adam - train loss: 4.05 - valid loss: 5.52, valid ACC: 9.71e-01
2024-01-21 02:05:38,044 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-05-37+00
2024-01-21 02:05:39,467 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+14-33-35+00
2024-01-21 02:05:39,481 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-17-48+00
2024-01-21 02:05:39,481 - speechbrain.utils.epoch_loop - INFO - Going into epoch 87
2024-01-21 02:35:41,197 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-35-39+00
2024-01-21 02:35:42,640 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-04-16+00
2024-01-21 02:37:12,366 - speechbrain.utils.train_logger - INFO - epoch: 87, lr: 3.63e-04, steps: 190040, optimizer: Adam - train loss: 4.02 - valid loss: 5.58, valid ACC: 9.72e-01
2024-01-21 02:37:13,347 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-37-12+00
2024-01-21 02:37:14,724 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+20-49-05+00
2024-01-21 02:37:14,728 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+10-56-24+00
2024-01-21 02:37:14,728 - speechbrain.utils.epoch_loop - INFO - Going into epoch 88
2024-01-21 03:07:16,348 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-07-14+00
2024-01-21 03:07:17,715 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-35-39+00
2024-01-21 03:08:50,228 - speechbrain.utils.train_logger - INFO - epoch: 88, lr: 3.61e-04, steps: 192237, optimizer: Adam - train loss: 4.01 - valid loss: 5.68, valid ACC: 9.71e-01
2024-01-21 03:08:51,210 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-08-50+00
2024-01-21 03:08:52,513 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+18-13-23+00
2024-01-21 03:08:52,528 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+21-20-32+00
2024-01-21 03:08:52,528 - speechbrain.utils.epoch_loop - INFO - Going into epoch 89
2024-01-21 03:38:53,970 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-38-52+00
2024-01-21 03:38:55,308 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-07-14+00
2024-01-21 03:40:28,313 - speechbrain.utils.train_logger - INFO - epoch: 89, lr: 3.59e-04, steps: 194434, optimizer: Adam - train loss: 3.96 - valid loss: 5.62, valid ACC: 9.71e-01
2024-01-21 03:40:29,315 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-40-28+00
2024-01-21 03:40:30,561 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+11-58-21+00
2024-01-21 03:40:30,562 - speechbrain.utils.epoch_loop - INFO - Going into epoch 90
2024-01-21 04:10:31,811 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+04-10-30+00
2024-01-21 04:10:33,147 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-38-52+00
2024-01-21 05:15:17,502 - speechbrain.utils.train_logger - INFO - epoch: 90, lr: 3.57e-04, steps: 196631, optimizer: Adam - train loss: 3.94 - valid loss: 5.63, valid ACC: 9.71e-01, valid WER: 3.43
2024-01-21 05:15:18,437 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-15-17+00
2024-01-21 05:15:19,691 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-27-36+00
2024-01-21 05:15:19,691 - speechbrain.utils.epoch_loop - INFO - Going into epoch 91
2024-01-21 05:45:20,758 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-45-19+00
2024-01-21 05:45:22,121 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+04-10-30+00
2024-01-21 05:48:13,429 - speechbrain.utils.train_logger - INFO - epoch: 91, lr: 3.55e-04, steps: 198828, optimizer: Adam - train loss: 3.92 - valid loss: 5.63, valid ACC: 9.71e-01
2024-01-21 05:48:14,474 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-48-13+00
2024-01-21 05:48:15,760 - speechbrain.utils.epoch_loop - INFO - Going into epoch 92
2024-01-21 06:18:17,293 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-18-16+00
2024-01-21 06:18:18,755 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-45-19+00
2024-01-21 06:19:47,264 - speechbrain.utils.train_logger - INFO - epoch: 92, lr: 3.53e-04, steps: 201025, optimizer: Adam - train loss: 3.89 - valid loss: 5.67, valid ACC: 9.71e-01
2024-01-21 06:19:48,215 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-19-47+00
2024-01-21 06:19:49,619 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+00-31-07+00
2024-01-21 06:19:49,620 - speechbrain.utils.epoch_loop - INFO - Going into epoch 93
2024-01-21 06:49:50,885 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-49-49+00
2024-01-21 06:49:52,351 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-18-16+00
2024-01-21 06:51:15,820 - speechbrain.utils.train_logger - INFO - epoch: 93, lr: 3.51e-04, steps: 203222, optimizer: Adam - train loss: 3.89 - valid loss: 5.54, valid ACC: 9.71e-01
2024-01-21 06:51:16,756 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-51-15+00
2024-01-21 06:51:18,190 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+15-04-40+00
2024-01-21 06:51:18,191 - speechbrain.utils.epoch_loop - INFO - Going into epoch 94
2024-01-21 07:21:19,415 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-21-18+00
2024-01-21 07:21:20,930 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-49-49+00
2024-01-21 07:22:44,355 - speechbrain.utils.train_logger - INFO - epoch: 94, lr: 3.49e-04, steps: 205419, optimizer: Adam - train loss: 3.85 - valid loss: 5.65, valid ACC: 9.71e-01
2024-01-21 07:22:45,424 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-22-44+00
2024-01-21 07:22:46,888 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+23-59-21+00
2024-01-21 07:22:46,902 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-34-14+00
2024-01-21 07:22:46,903 - speechbrain.utils.epoch_loop - INFO - Going into epoch 95
2024-01-21 07:52:48,311 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-52-47+00
2024-01-21 07:52:49,752 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-21-18+00
2024-01-21 07:54:12,848 - speechbrain.utils.train_logger - INFO - epoch: 95, lr: 3.47e-04, steps: 207616, optimizer: Adam - train loss: 3.82 - valid loss: 5.73, valid ACC: 9.71e-01
2024-01-21 07:54:13,819 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-54-12+00
2024-01-21 07:54:15,202 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+19-46-42+00
2024-01-21 07:54:15,203 - speechbrain.utils.epoch_loop - INFO - Going into epoch 96
2024-01-21 08:24:16,404 - speechbrain.utils.checkpoints - DEBUG - Saved an intra-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-24-15+00
2024-01-21 08:24:17,860 - speechbrain.utils.checkpoints - DEBUG - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-52-47+00
2024-01-21 08:25:34,219 - speechbrain.utils.train_logger - INFO - epoch: 96, lr: 3.45e-04, steps: 209813, optimizer: Adam - train loss: 3.81 - valid loss: 5.66, valid ACC: 9.71e-01
2024-01-21 08:25:35,359 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-25-34+00
2024-01-21 08:25:36,778 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+01-02-46+00
2024-01-21 08:25:36,778 - speechbrain.utils.epoch_loop - INFO - Going into epoch 97
2024-01-21 08:56:33,057 - speechbrain.utils.train_logger - INFO - epoch: 97, lr: 3.43e-04, steps: 212010, optimizer: Adam - train loss: 3.78 - valid loss: 5.44, valid ACC: 9.72e-01
2024-01-21 08:56:34,016 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-56-33+00
2024-01-21 08:56:35,450 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-05-37+00
2024-01-21 08:56:35,468 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-08-50+00
2024-01-21 08:56:35,468 - speechbrain.utils.epoch_loop - INFO - Going into epoch 98
2024-01-21 09:27:44,944 - speechbrain.utils.train_logger - INFO - epoch: 98, lr: 3.42e-04, steps: 214207, optimizer: Adam - train loss: 3.77 - valid loss: 5.73, valid ACC: 9.71e-01
2024-01-21 09:27:45,957 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+09-27-44+00
2024-01-21 09:27:47,288 - speechbrain.utils.epoch_loop - INFO - Going into epoch 99
2024-01-21 09:58:49,379 - speechbrain.utils.train_logger - INFO - epoch: 99, lr: 3.40e-04, steps: 216404, optimizer: Adam - train loss: 3.75 - valid loss: 5.67, valid ACC: 9.71e-01
2024-01-21 09:58:50,366 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+09-58-49+00
2024-01-21 09:58:51,821 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-20+22-55-53+00
2024-01-21 09:58:51,822 - speechbrain.utils.epoch_loop - INFO - Going into epoch 100
2024-01-21 11:33:33,152 - speechbrain.utils.train_logger - INFO - epoch: 100, lr: 3.38e-04, steps: 218601, optimizer: Adam - train loss: 3.72 - valid loss: 5.75, valid ACC: 9.71e-01, valid WER: 3.38
2024-01-21 11:33:34,103 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+11-33-33+00
2024-01-21 11:33:35,556 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+11-33-33+00
2024-01-21 11:33:35,687 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2024-01-21 13:27:15,666 - speechbrain.utils.train_logger - INFO - Epoch loaded: 100 - test loss: 5.61, test ACC: 9.74e-01, test WER: 2.34
2024-01-21 13:27:16,737 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+13-27-15+00
2024-01-21 13:27:18,332 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-56-33+00
2024-01-21 13:27:18,353 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+03-40-28+00
2024-01-21 13:27:18,376 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-19-47+00
2024-01-21 13:27:18,400 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-22-44+00
2024-01-21 13:27:18,454 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-25-34+00
2024-01-21 13:27:18,483 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+06-51-15+00
2024-01-21 13:27:18,518 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+07-54-12+00
2024-01-21 13:27:18,546 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+02-37-12+00
2024-01-21 13:27:18,581 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-15-17+00
2024-01-21 13:27:18,693 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+11-33-33+00
2024-01-21 13:27:18,696 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+09-58-49+00
2024-01-21 13:27:18,718 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+05-48-13+00
2024-01-21 13:27:18,721 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+08-24-15+00
2024-01-21 13:27:18,757 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+09-27-44+00
2024-01-21 13:27:18,759 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+13-27-15+00
2024-01-21 13:27:18,916 - root - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2024-01-21 15:13:14,241 - speechbrain.utils.train_logger - INFO - Epoch loaded: 100 - test loss: 5.87, test ACC: 9.40e-01, test WER: 5.57
2024-01-21 15:13:15,273 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-21 15:13:15,590 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results/transformer/74443/save/CKPT+2024-01-21+13-27-15+00
2024-01-29 09:21:49,631 - speechbrain.core - INFO - Beginning experiment!
2024-01-29 09:21:49,632 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-29 09:21:50,076 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-29 09:21:50,186 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-29 09:21:51,987 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-29 09:21:51,987 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-29 09:21:51,987 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-29 09:21:51,988 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-29 09:21:55,419 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 479, in <module>
    torch.cuda.set_device(local_rank)
                          ^^^^^^^^^^
NameError: name 'local_rank' is not defined
2024-01-29 09:22:32,108 - speechbrain.core - INFO - Beginning experiment!
2024-01-29 09:22:32,108 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-29 09:22:32,523 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-29 09:22:32,676 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-29 09:22:34,457 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-29 09:22:34,458 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-29 09:22:34,458 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-29 09:22:34,458 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-29 09:22:36,344 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-29 09:22:36,344 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-29 09:22:36,344 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-29 09:22:36,709 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-29 09:22:36,714 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-29 09:22:37,145 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	Missing key(s) in state_dict: "1.hubert.model.masked_spec_embed", "1.hubert.model.feature_extractor.conv_layers.0.conv.weight", "1.hubert.model.feature_extractor.conv_layers.0.layer_norm.weight", "1.hubert.model.feature_extractor.conv_layers.0.layer_norm.bias", "1.hubert.model.feature_extractor.conv_layers.1.conv.weight", "1.hubert.model.feature_extractor.conv_layers.2.conv.weight", "1.hubert.model.feature_extractor.conv_layers.3.conv.weight", "1.hubert.model.feature_extractor.conv_layers.4.conv.weight", "1.hubert.model.feature_extractor.conv_layers.5.conv.weight", "1.hubert.model.feature_extractor.conv_layers.6.conv.weight", "1.hubert.model.feature_projection.layer_norm.weight", "1.hubert.model.feature_projection.layer_norm.bias", "1.hubert.model.feature_projection.projection.weight", "1.hubert.model.feature_projection.projection.bias", "1.hubert.model.encoder.pos_conv_embed.conv.bias", "1.hubert.model.encoder.pos_conv_embed.conv.weight_g", "1.hubert.model.encoder.pos_conv_embed.conv.weight_v", "1.hubert.model.encoder.layer_norm.weight", "1.hubert.model.encoder.layer_norm.bias", "1.hubert.model.encoder.layers.0.attention.k_proj.weight", "1.hubert.model.encoder.layers.0.attention.k_proj.bias", "1.hubert.model.encoder.layers.0.attention.v_proj.weight", "1.hubert.model.encoder.layers.0.attention.v_proj.bias", "1.hubert.model.encoder.layers.0.attention.q_proj.weight", "1.hubert.model.encoder.layers.0.attention.q_proj.bias", "1.hubert.model.encoder.layers.0.attention.out_proj.weight", "1.hubert.model.encoder.layers.0.attention.out_proj.bias", "1.hubert.model.encoder.layers.0.layer_norm.weight", "1.hubert.model.encoder.layers.0.layer_norm.bias", "1.hubert.model.encoder.layers.0.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.0.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.0.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.0.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.0.final_layer_norm.weight", "1.hubert.model.encoder.layers.0.final_layer_norm.bias", "1.hubert.model.encoder.layers.1.attention.k_proj.weight", "1.hubert.model.encoder.layers.1.attention.k_proj.bias", "1.hubert.model.encoder.layers.1.attention.v_proj.weight", "1.hubert.model.encoder.layers.1.attention.v_proj.bias", "1.hubert.model.encoder.layers.1.attention.q_proj.weight", "1.hubert.model.encoder.layers.1.attention.q_proj.bias", "1.hubert.model.encoder.layers.1.attention.out_proj.weight", "1.hubert.model.encoder.layers.1.attention.out_proj.bias", "1.hubert.model.encoder.layers.1.layer_norm.weight", "1.hubert.model.encoder.layers.1.layer_norm.bias", "1.hubert.model.encoder.layers.1.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.1.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.1.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.1.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.1.final_layer_norm.weight", "1.hubert.model.encoder.layers.1.final_layer_norm.bias", "1.hubert.model.encoder.layers.2.attention.k_proj.weight", "1.hubert.model.encoder.layers.2.attention.k_proj.bias", "1.hubert.model.encoder.layers.2.attention.v_proj.weight", "1.hubert.model.encoder.layers.2.attention.v_proj.bias", "1.hubert.model.encoder.layers.2.attention.q_proj.weight", "1.hubert.model.encoder.layers.2.attention.q_proj.bias", "1.hubert.model.encoder.layers.2.attention.out_proj.weight", "1.hubert.model.encoder.layers.2.attention.out_proj.bias", "1.hubert.model.encoder.layers.2.layer_norm.weight", "1.hubert.model.encoder.layers.2.layer_norm.bias", "1.hubert.model.encoder.layers.2.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.2.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.2.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.2.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.2.final_layer_norm.weight", "1.hubert.model.encoder.layers.2.final_layer_norm.bias", "1.hubert.model.encoder.layers.3.attention.k_proj.weight", "1.hubert.model.encoder.layers.3.attention.k_proj.bias", "1.hubert.model.encoder.layers.3.attention.v_proj.weight", "1.hubert.model.encoder.layers.3.attention.v_proj.bias", "1.hubert.model.encoder.layers.3.attention.q_proj.weight", "1.hubert.model.encoder.layers.3.attention.q_proj.bias", "1.hubert.model.encoder.layers.3.attention.out_proj.weight", "1.hubert.model.encoder.layers.3.attention.out_proj.bias", "1.hubert.model.encoder.layers.3.layer_norm.weight", "1.hubert.model.encoder.layers.3.layer_norm.bias", "1.hubert.model.encoder.layers.3.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.3.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.3.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.3.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.3.final_layer_norm.weight", "1.hubert.model.encoder.layers.3.final_layer_norm.bias", "1.hubert.model.encoder.layers.4.attention.k_proj.weight", "1.hubert.model.encoder.layers.4.attention.k_proj.bias", "1.hubert.model.encoder.layers.4.attention.v_proj.weight", "1.hubert.model.encoder.layers.4.attention.v_proj.bias", "1.hubert.model.encoder.layers.4.attention.q_proj.weight", "1.hubert.model.encoder.layers.4.attention.q_proj.bias", "1.hubert.model.encoder.layers.4.attention.out_proj.weight", "1.hubert.model.encoder.layers.4.attention.out_proj.bias", "1.hubert.model.encoder.layers.4.layer_norm.weight", "1.hubert.model.encoder.layers.4.layer_norm.bias", "1.hubert.model.encoder.layers.4.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.4.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.4.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.4.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.4.final_layer_norm.weight", "1.hubert.model.encoder.layers.4.final_layer_norm.bias", "1.hubert.model.encoder.layers.5.attention.k_proj.weight", "1.hubert.model.encoder.layers.5.attention.k_proj.bias", "1.hubert.model.encoder.layers.5.attention.v_proj.weight", "1.hubert.model.encoder.layers.5.attention.v_proj.bias", "1.hubert.model.encoder.layers.5.attention.q_proj.weight", "1.hubert.model.encoder.layers.5.attention.q_proj.bias", "1.hubert.model.encoder.layers.5.attention.out_proj.weight", "1.hubert.model.encoder.layers.5.attention.out_proj.bias", "1.hubert.model.encoder.layers.5.layer_norm.weight", "1.hubert.model.encoder.layers.5.layer_norm.bias", "1.hubert.model.encoder.layers.5.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.5.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.5.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.5.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.5.final_layer_norm.weight", "1.hubert.model.encoder.layers.5.final_layer_norm.bias", "1.hubert.model.encoder.layers.6.attention.k_proj.weight", "1.hubert.model.encoder.layers.6.attention.k_proj.bias", "1.hubert.model.encoder.layers.6.attention.v_proj.weight", "1.hubert.model.encoder.layers.6.attention.v_proj.bias", "1.hubert.model.encoder.layers.6.attention.q_proj.weight", "1.hubert.model.encoder.layers.6.attention.q_proj.bias", "1.hubert.model.encoder.layers.6.attention.out_proj.weight", "1.hubert.model.encoder.layers.6.attention.out_proj.bias", "1.hubert.model.encoder.layers.6.layer_norm.weight", "1.hubert.model.encoder.layers.6.layer_norm.bias", "1.hubert.model.encoder.layers.6.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.6.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.6.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.6.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.6.final_layer_norm.weight", "1.hubert.model.encoder.layers.6.final_layer_norm.bias", "1.hubert.model.encoder.layers.7.attention.k_proj.weight", "1.hubert.model.encoder.layers.7.attention.k_proj.bias", "1.hubert.model.encoder.layers.7.attention.v_proj.weight", "1.hubert.model.encoder.layers.7.attention.v_proj.bias", "1.hubert.model.encoder.layers.7.attention.q_proj.weight", "1.hubert.model.encoder.layers.7.attention.q_proj.bias", "1.hubert.model.encoder.layers.7.attention.out_proj.weight", "1.hubert.model.encoder.layers.7.attention.out_proj.bias", "1.hubert.model.encoder.layers.7.layer_norm.weight", "1.hubert.model.encoder.layers.7.layer_norm.bias", "1.hubert.model.encoder.layers.7.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.7.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.7.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.7.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.7.final_layer_norm.weight", "1.hubert.model.encoder.layers.7.final_layer_norm.bias", "1.hubert.model.encoder.layers.8.attention.k_proj.weight", "1.hubert.model.encoder.layers.8.attention.k_proj.bias", "1.hubert.model.encoder.layers.8.attention.v_proj.weight", "1.hubert.model.encoder.layers.8.attention.v_proj.bias", "1.hubert.model.encoder.layers.8.attention.q_proj.weight", "1.hubert.model.encoder.layers.8.attention.q_proj.bias", "1.hubert.model.encoder.layers.8.attention.out_proj.weight", "1.hubert.model.encoder.layers.8.attention.out_proj.bias", "1.hubert.model.encoder.layers.8.layer_norm.weight", "1.hubert.model.encoder.layers.8.layer_norm.bias", "1.hubert.model.encoder.layers.8.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.8.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.8.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.8.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.8.final_layer_norm.weight", "1.hubert.model.encoder.layers.8.final_layer_norm.bias", "1.hubert.model.encoder.layers.9.attention.k_proj.weight", "1.hubert.model.encoder.layers.9.attention.k_proj.bias", "1.hubert.model.encoder.layers.9.attention.v_proj.weight", "1.hubert.model.encoder.layers.9.attention.v_proj.bias", "1.hubert.model.encoder.layers.9.attention.q_proj.weight", "1.hubert.model.encoder.layers.9.attention.q_proj.bias", "1.hubert.model.encoder.layers.9.attention.out_proj.weight", "1.hubert.model.encoder.layers.9.attention.out_proj.bias", "1.hubert.model.encoder.layers.9.layer_norm.weight", "1.hubert.model.encoder.layers.9.layer_norm.bias", "1.hubert.model.encoder.layers.9.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.9.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.9.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.9.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.9.final_layer_norm.weight", "1.hubert.model.encoder.layers.9.final_layer_norm.bias", "1.hubert.model.encoder.layers.10.attention.k_proj.weight", "1.hubert.model.encoder.layers.10.attention.k_proj.bias", "1.hubert.model.encoder.layers.10.attention.v_proj.weight", "1.hubert.model.encoder.layers.10.attention.v_proj.bias", "1.hubert.model.encoder.layers.10.attention.q_proj.weight", "1.hubert.model.encoder.layers.10.attention.q_proj.bias", "1.hubert.model.encoder.layers.10.attention.out_proj.weight", "1.hubert.model.encoder.layers.10.attention.out_proj.bias", "1.hubert.model.encoder.layers.10.layer_norm.weight", "1.hubert.model.encoder.layers.10.layer_norm.bias", "1.hubert.model.encoder.layers.10.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.10.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.10.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.10.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.10.final_layer_norm.weight", "1.hubert.model.encoder.layers.10.final_layer_norm.bias", "1.hubert.model.encoder.layers.11.attention.k_proj.weight", "1.hubert.model.encoder.layers.11.attention.k_proj.bias", "1.hubert.model.encoder.layers.11.attention.v_proj.weight", "1.hubert.model.encoder.layers.11.attention.v_proj.bias", "1.hubert.model.encoder.layers.11.attention.q_proj.weight", "1.hubert.model.encoder.layers.11.attention.q_proj.bias", "1.hubert.model.encoder.layers.11.attention.out_proj.weight", "1.hubert.model.encoder.layers.11.attention.out_proj.bias", "1.hubert.model.encoder.layers.11.layer_norm.weight", "1.hubert.model.encoder.layers.11.layer_norm.bias", "1.hubert.model.encoder.layers.11.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.11.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.11.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.11.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.11.final_layer_norm.weight", "1.hubert.model.encoder.layers.11.final_layer_norm.bias". 
2024-01-30 05:23:36,739 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 05:23:36,740 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 05:23:37,317 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 05:23:37,441 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 05:23:39,209 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 05:23:39,209 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 05:23:39,210 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 05:23:39,210 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 05:23:42,763 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 05:23:42,763 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 05:23:42,763 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 05:23:43,000 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 05:23:43,004 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 05:23:43,476 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	Missing key(s) in state_dict: "1.hubert.model.masked_spec_embed", "1.hubert.model.feature_extractor.conv_layers.0.conv.weight", "1.hubert.model.feature_extractor.conv_layers.0.layer_norm.weight", "1.hubert.model.feature_extractor.conv_layers.0.layer_norm.bias", "1.hubert.model.feature_extractor.conv_layers.1.conv.weight", "1.hubert.model.feature_extractor.conv_layers.2.conv.weight", "1.hubert.model.feature_extractor.conv_layers.3.conv.weight", "1.hubert.model.feature_extractor.conv_layers.4.conv.weight", "1.hubert.model.feature_extractor.conv_layers.5.conv.weight", "1.hubert.model.feature_extractor.conv_layers.6.conv.weight", "1.hubert.model.feature_projection.layer_norm.weight", "1.hubert.model.feature_projection.layer_norm.bias", "1.hubert.model.feature_projection.projection.weight", "1.hubert.model.feature_projection.projection.bias", "1.hubert.model.encoder.pos_conv_embed.conv.bias", "1.hubert.model.encoder.pos_conv_embed.conv.weight_g", "1.hubert.model.encoder.pos_conv_embed.conv.weight_v", "1.hubert.model.encoder.layer_norm.weight", "1.hubert.model.encoder.layer_norm.bias", "1.hubert.model.encoder.layers.0.attention.k_proj.weight", "1.hubert.model.encoder.layers.0.attention.k_proj.bias", "1.hubert.model.encoder.layers.0.attention.v_proj.weight", "1.hubert.model.encoder.layers.0.attention.v_proj.bias", "1.hubert.model.encoder.layers.0.attention.q_proj.weight", "1.hubert.model.encoder.layers.0.attention.q_proj.bias", "1.hubert.model.encoder.layers.0.attention.out_proj.weight", "1.hubert.model.encoder.layers.0.attention.out_proj.bias", "1.hubert.model.encoder.layers.0.layer_norm.weight", "1.hubert.model.encoder.layers.0.layer_norm.bias", "1.hubert.model.encoder.layers.0.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.0.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.0.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.0.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.0.final_layer_norm.weight", "1.hubert.model.encoder.layers.0.final_layer_norm.bias", "1.hubert.model.encoder.layers.1.attention.k_proj.weight", "1.hubert.model.encoder.layers.1.attention.k_proj.bias", "1.hubert.model.encoder.layers.1.attention.v_proj.weight", "1.hubert.model.encoder.layers.1.attention.v_proj.bias", "1.hubert.model.encoder.layers.1.attention.q_proj.weight", "1.hubert.model.encoder.layers.1.attention.q_proj.bias", "1.hubert.model.encoder.layers.1.attention.out_proj.weight", "1.hubert.model.encoder.layers.1.attention.out_proj.bias", "1.hubert.model.encoder.layers.1.layer_norm.weight", "1.hubert.model.encoder.layers.1.layer_norm.bias", "1.hubert.model.encoder.layers.1.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.1.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.1.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.1.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.1.final_layer_norm.weight", "1.hubert.model.encoder.layers.1.final_layer_norm.bias", "1.hubert.model.encoder.layers.2.attention.k_proj.weight", "1.hubert.model.encoder.layers.2.attention.k_proj.bias", "1.hubert.model.encoder.layers.2.attention.v_proj.weight", "1.hubert.model.encoder.layers.2.attention.v_proj.bias", "1.hubert.model.encoder.layers.2.attention.q_proj.weight", "1.hubert.model.encoder.layers.2.attention.q_proj.bias", "1.hubert.model.encoder.layers.2.attention.out_proj.weight", "1.hubert.model.encoder.layers.2.attention.out_proj.bias", "1.hubert.model.encoder.layers.2.layer_norm.weight", "1.hubert.model.encoder.layers.2.layer_norm.bias", "1.hubert.model.encoder.layers.2.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.2.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.2.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.2.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.2.final_layer_norm.weight", "1.hubert.model.encoder.layers.2.final_layer_norm.bias", "1.hubert.model.encoder.layers.3.attention.k_proj.weight", "1.hubert.model.encoder.layers.3.attention.k_proj.bias", "1.hubert.model.encoder.layers.3.attention.v_proj.weight", "1.hubert.model.encoder.layers.3.attention.v_proj.bias", "1.hubert.model.encoder.layers.3.attention.q_proj.weight", "1.hubert.model.encoder.layers.3.attention.q_proj.bias", "1.hubert.model.encoder.layers.3.attention.out_proj.weight", "1.hubert.model.encoder.layers.3.attention.out_proj.bias", "1.hubert.model.encoder.layers.3.layer_norm.weight", "1.hubert.model.encoder.layers.3.layer_norm.bias", "1.hubert.model.encoder.layers.3.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.3.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.3.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.3.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.3.final_layer_norm.weight", "1.hubert.model.encoder.layers.3.final_layer_norm.bias", "1.hubert.model.encoder.layers.4.attention.k_proj.weight", "1.hubert.model.encoder.layers.4.attention.k_proj.bias", "1.hubert.model.encoder.layers.4.attention.v_proj.weight", "1.hubert.model.encoder.layers.4.attention.v_proj.bias", "1.hubert.model.encoder.layers.4.attention.q_proj.weight", "1.hubert.model.encoder.layers.4.attention.q_proj.bias", "1.hubert.model.encoder.layers.4.attention.out_proj.weight", "1.hubert.model.encoder.layers.4.attention.out_proj.bias", "1.hubert.model.encoder.layers.4.layer_norm.weight", "1.hubert.model.encoder.layers.4.layer_norm.bias", "1.hubert.model.encoder.layers.4.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.4.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.4.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.4.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.4.final_layer_norm.weight", "1.hubert.model.encoder.layers.4.final_layer_norm.bias", "1.hubert.model.encoder.layers.5.attention.k_proj.weight", "1.hubert.model.encoder.layers.5.attention.k_proj.bias", "1.hubert.model.encoder.layers.5.attention.v_proj.weight", "1.hubert.model.encoder.layers.5.attention.v_proj.bias", "1.hubert.model.encoder.layers.5.attention.q_proj.weight", "1.hubert.model.encoder.layers.5.attention.q_proj.bias", "1.hubert.model.encoder.layers.5.attention.out_proj.weight", "1.hubert.model.encoder.layers.5.attention.out_proj.bias", "1.hubert.model.encoder.layers.5.layer_norm.weight", "1.hubert.model.encoder.layers.5.layer_norm.bias", "1.hubert.model.encoder.layers.5.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.5.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.5.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.5.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.5.final_layer_norm.weight", "1.hubert.model.encoder.layers.5.final_layer_norm.bias", "1.hubert.model.encoder.layers.6.attention.k_proj.weight", "1.hubert.model.encoder.layers.6.attention.k_proj.bias", "1.hubert.model.encoder.layers.6.attention.v_proj.weight", "1.hubert.model.encoder.layers.6.attention.v_proj.bias", "1.hubert.model.encoder.layers.6.attention.q_proj.weight", "1.hubert.model.encoder.layers.6.attention.q_proj.bias", "1.hubert.model.encoder.layers.6.attention.out_proj.weight", "1.hubert.model.encoder.layers.6.attention.out_proj.bias", "1.hubert.model.encoder.layers.6.layer_norm.weight", "1.hubert.model.encoder.layers.6.layer_norm.bias", "1.hubert.model.encoder.layers.6.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.6.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.6.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.6.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.6.final_layer_norm.weight", "1.hubert.model.encoder.layers.6.final_layer_norm.bias", "1.hubert.model.encoder.layers.7.attention.k_proj.weight", "1.hubert.model.encoder.layers.7.attention.k_proj.bias", "1.hubert.model.encoder.layers.7.attention.v_proj.weight", "1.hubert.model.encoder.layers.7.attention.v_proj.bias", "1.hubert.model.encoder.layers.7.attention.q_proj.weight", "1.hubert.model.encoder.layers.7.attention.q_proj.bias", "1.hubert.model.encoder.layers.7.attention.out_proj.weight", "1.hubert.model.encoder.layers.7.attention.out_proj.bias", "1.hubert.model.encoder.layers.7.layer_norm.weight", "1.hubert.model.encoder.layers.7.layer_norm.bias", "1.hubert.model.encoder.layers.7.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.7.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.7.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.7.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.7.final_layer_norm.weight", "1.hubert.model.encoder.layers.7.final_layer_norm.bias", "1.hubert.model.encoder.layers.8.attention.k_proj.weight", "1.hubert.model.encoder.layers.8.attention.k_proj.bias", "1.hubert.model.encoder.layers.8.attention.v_proj.weight", "1.hubert.model.encoder.layers.8.attention.v_proj.bias", "1.hubert.model.encoder.layers.8.attention.q_proj.weight", "1.hubert.model.encoder.layers.8.attention.q_proj.bias", "1.hubert.model.encoder.layers.8.attention.out_proj.weight", "1.hubert.model.encoder.layers.8.attention.out_proj.bias", "1.hubert.model.encoder.layers.8.layer_norm.weight", "1.hubert.model.encoder.layers.8.layer_norm.bias", "1.hubert.model.encoder.layers.8.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.8.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.8.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.8.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.8.final_layer_norm.weight", "1.hubert.model.encoder.layers.8.final_layer_norm.bias", "1.hubert.model.encoder.layers.9.attention.k_proj.weight", "1.hubert.model.encoder.layers.9.attention.k_proj.bias", "1.hubert.model.encoder.layers.9.attention.v_proj.weight", "1.hubert.model.encoder.layers.9.attention.v_proj.bias", "1.hubert.model.encoder.layers.9.attention.q_proj.weight", "1.hubert.model.encoder.layers.9.attention.q_proj.bias", "1.hubert.model.encoder.layers.9.attention.out_proj.weight", "1.hubert.model.encoder.layers.9.attention.out_proj.bias", "1.hubert.model.encoder.layers.9.layer_norm.weight", "1.hubert.model.encoder.layers.9.layer_norm.bias", "1.hubert.model.encoder.layers.9.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.9.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.9.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.9.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.9.final_layer_norm.weight", "1.hubert.model.encoder.layers.9.final_layer_norm.bias", "1.hubert.model.encoder.layers.10.attention.k_proj.weight", "1.hubert.model.encoder.layers.10.attention.k_proj.bias", "1.hubert.model.encoder.layers.10.attention.v_proj.weight", "1.hubert.model.encoder.layers.10.attention.v_proj.bias", "1.hubert.model.encoder.layers.10.attention.q_proj.weight", "1.hubert.model.encoder.layers.10.attention.q_proj.bias", "1.hubert.model.encoder.layers.10.attention.out_proj.weight", "1.hubert.model.encoder.layers.10.attention.out_proj.bias", "1.hubert.model.encoder.layers.10.layer_norm.weight", "1.hubert.model.encoder.layers.10.layer_norm.bias", "1.hubert.model.encoder.layers.10.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.10.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.10.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.10.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.10.final_layer_norm.weight", "1.hubert.model.encoder.layers.10.final_layer_norm.bias", "1.hubert.model.encoder.layers.11.attention.k_proj.weight", "1.hubert.model.encoder.layers.11.attention.k_proj.bias", "1.hubert.model.encoder.layers.11.attention.v_proj.weight", "1.hubert.model.encoder.layers.11.attention.v_proj.bias", "1.hubert.model.encoder.layers.11.attention.q_proj.weight", "1.hubert.model.encoder.layers.11.attention.q_proj.bias", "1.hubert.model.encoder.layers.11.attention.out_proj.weight", "1.hubert.model.encoder.layers.11.attention.out_proj.bias", "1.hubert.model.encoder.layers.11.layer_norm.weight", "1.hubert.model.encoder.layers.11.layer_norm.bias", "1.hubert.model.encoder.layers.11.feed_forward.intermediate_dense.weight", "1.hubert.model.encoder.layers.11.feed_forward.intermediate_dense.bias", "1.hubert.model.encoder.layers.11.feed_forward.output_dense.weight", "1.hubert.model.encoder.layers.11.feed_forward.output_dense.bias", "1.hubert.model.encoder.layers.11.final_layer_norm.weight", "1.hubert.model.encoder.layers.11.final_layer_norm.bias". 
2024-01-30 06:26:26,136 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:26:26,137 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:26:26,540 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:26:26,685 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:26:28,484 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:26:28,484 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:26:28,485 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:26:28,485 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:26:32,188 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:26:32,188 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:26:32,188 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:26:32,679 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-30 06:26:32,683 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:26:36,407 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:26:46,877 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 541, in <module>
    asr_brain.evaluate(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1543, in evaluate
    loss = self.evaluate_batch(batch, stage=Stage.TEST)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 179, in evaluate_batch
    predictions = self.compute_forward(batch, stage=stage)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 86, in compute_forward
    enc_out, src_key_padding_mask = self.modules.wav2vec2(src, wav_lens)
                                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'ModuleDict' object has no attribute 'wav2vec2'
2024-01-30 06:27:52,681 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:27:52,682 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:27:53,074 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:27:53,206 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:27:55,115 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:27:55,115 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:27:55,116 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:27:55,116 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:27:58,515 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:27:58,516 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:27:58,516 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:27:59,423 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:27:59,426 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:28:02,738 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
TypeError: Optimizer.load_state_dict() got an unexpected keyword argument 'strict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 98, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device))
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/optim/optimizer.py", line 390, in load_state_dict
    raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2024-01-30 06:29:42,099 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:29:42,100 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:29:42,495 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:29:42,647 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:29:44,420 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:29:44,420 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:29:44,421 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:29:44,421 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:29:47,482 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:29:47,483 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:29:47,483 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:29:48,443 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:29:48,447 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:29:48,935 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
	Missing key(s) in state_dict: "2.model.masked_spec_embed", "2.model.feature_extractor.conv_layers.0.conv.weight", "2.model.feature_extractor.conv_layers.0.layer_norm.weight", "2.model.feature_extractor.conv_layers.0.layer_norm.bias", "2.model.feature_extractor.conv_layers.1.conv.weight", "2.model.feature_extractor.conv_layers.2.conv.weight", "2.model.feature_extractor.conv_layers.3.conv.weight", "2.model.feature_extractor.conv_layers.4.conv.weight", "2.model.feature_extractor.conv_layers.5.conv.weight", "2.model.feature_extractor.conv_layers.6.conv.weight", "2.model.feature_projection.layer_norm.weight", "2.model.feature_projection.layer_norm.bias", "2.model.feature_projection.projection.weight", "2.model.feature_projection.projection.bias", "2.model.encoder.pos_conv_embed.conv.bias", "2.model.encoder.pos_conv_embed.conv.weight_g", "2.model.encoder.pos_conv_embed.conv.weight_v", "2.model.encoder.layer_norm.weight", "2.model.encoder.layer_norm.bias", "2.model.encoder.layers.0.attention.k_proj.weight", "2.model.encoder.layers.0.attention.k_proj.bias", "2.model.encoder.layers.0.attention.v_proj.weight", "2.model.encoder.layers.0.attention.v_proj.bias", "2.model.encoder.layers.0.attention.q_proj.weight", "2.model.encoder.layers.0.attention.q_proj.bias", "2.model.encoder.layers.0.attention.out_proj.weight", "2.model.encoder.layers.0.attention.out_proj.bias", "2.model.encoder.layers.0.layer_norm.weight", "2.model.encoder.layers.0.layer_norm.bias", "2.model.encoder.layers.0.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.0.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.0.feed_forward.output_dense.weight", "2.model.encoder.layers.0.feed_forward.output_dense.bias", "2.model.encoder.layers.0.final_layer_norm.weight", "2.model.encoder.layers.0.final_layer_norm.bias", "2.model.encoder.layers.1.attention.k_proj.weight", "2.model.encoder.layers.1.attention.k_proj.bias", "2.model.encoder.layers.1.attention.v_proj.weight", "2.model.encoder.layers.1.attention.v_proj.bias", "2.model.encoder.layers.1.attention.q_proj.weight", "2.model.encoder.layers.1.attention.q_proj.bias", "2.model.encoder.layers.1.attention.out_proj.weight", "2.model.encoder.layers.1.attention.out_proj.bias", "2.model.encoder.layers.1.layer_norm.weight", "2.model.encoder.layers.1.layer_norm.bias", "2.model.encoder.layers.1.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.1.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.1.feed_forward.output_dense.weight", "2.model.encoder.layers.1.feed_forward.output_dense.bias", "2.model.encoder.layers.1.final_layer_norm.weight", "2.model.encoder.layers.1.final_layer_norm.bias", "2.model.encoder.layers.2.attention.k_proj.weight", "2.model.encoder.layers.2.attention.k_proj.bias", "2.model.encoder.layers.2.attention.v_proj.weight", "2.model.encoder.layers.2.attention.v_proj.bias", "2.model.encoder.layers.2.attention.q_proj.weight", "2.model.encoder.layers.2.attention.q_proj.bias", "2.model.encoder.layers.2.attention.out_proj.weight", "2.model.encoder.layers.2.attention.out_proj.bias", "2.model.encoder.layers.2.layer_norm.weight", "2.model.encoder.layers.2.layer_norm.bias", "2.model.encoder.layers.2.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.2.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.2.feed_forward.output_dense.weight", "2.model.encoder.layers.2.feed_forward.output_dense.bias", "2.model.encoder.layers.2.final_layer_norm.weight", "2.model.encoder.layers.2.final_layer_norm.bias", "2.model.encoder.layers.3.attention.k_proj.weight", "2.model.encoder.layers.3.attention.k_proj.bias", "2.model.encoder.layers.3.attention.v_proj.weight", "2.model.encoder.layers.3.attention.v_proj.bias", "2.model.encoder.layers.3.attention.q_proj.weight", "2.model.encoder.layers.3.attention.q_proj.bias", "2.model.encoder.layers.3.attention.out_proj.weight", "2.model.encoder.layers.3.attention.out_proj.bias", "2.model.encoder.layers.3.layer_norm.weight", "2.model.encoder.layers.3.layer_norm.bias", "2.model.encoder.layers.3.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.3.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.3.feed_forward.output_dense.weight", "2.model.encoder.layers.3.feed_forward.output_dense.bias", "2.model.encoder.layers.3.final_layer_norm.weight", "2.model.encoder.layers.3.final_layer_norm.bias", "2.model.encoder.layers.4.attention.k_proj.weight", "2.model.encoder.layers.4.attention.k_proj.bias", "2.model.encoder.layers.4.attention.v_proj.weight", "2.model.encoder.layers.4.attention.v_proj.bias", "2.model.encoder.layers.4.attention.q_proj.weight", "2.model.encoder.layers.4.attention.q_proj.bias", "2.model.encoder.layers.4.attention.out_proj.weight", "2.model.encoder.layers.4.attention.out_proj.bias", "2.model.encoder.layers.4.layer_norm.weight", "2.model.encoder.layers.4.layer_norm.bias", "2.model.encoder.layers.4.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.4.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.4.feed_forward.output_dense.weight", "2.model.encoder.layers.4.feed_forward.output_dense.bias", "2.model.encoder.layers.4.final_layer_norm.weight", "2.model.encoder.layers.4.final_layer_norm.bias", "2.model.encoder.layers.5.attention.k_proj.weight", "2.model.encoder.layers.5.attention.k_proj.bias", "2.model.encoder.layers.5.attention.v_proj.weight", "2.model.encoder.layers.5.attention.v_proj.bias", "2.model.encoder.layers.5.attention.q_proj.weight", "2.model.encoder.layers.5.attention.q_proj.bias", "2.model.encoder.layers.5.attention.out_proj.weight", "2.model.encoder.layers.5.attention.out_proj.bias", "2.model.encoder.layers.5.layer_norm.weight", "2.model.encoder.layers.5.layer_norm.bias", "2.model.encoder.layers.5.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.5.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.5.feed_forward.output_dense.weight", "2.model.encoder.layers.5.feed_forward.output_dense.bias", "2.model.encoder.layers.5.final_layer_norm.weight", "2.model.encoder.layers.5.final_layer_norm.bias", "2.model.encoder.layers.6.attention.k_proj.weight", "2.model.encoder.layers.6.attention.k_proj.bias", "2.model.encoder.layers.6.attention.v_proj.weight", "2.model.encoder.layers.6.attention.v_proj.bias", "2.model.encoder.layers.6.attention.q_proj.weight", "2.model.encoder.layers.6.attention.q_proj.bias", "2.model.encoder.layers.6.attention.out_proj.weight", "2.model.encoder.layers.6.attention.out_proj.bias", "2.model.encoder.layers.6.layer_norm.weight", "2.model.encoder.layers.6.layer_norm.bias", "2.model.encoder.layers.6.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.6.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.6.feed_forward.output_dense.weight", "2.model.encoder.layers.6.feed_forward.output_dense.bias", "2.model.encoder.layers.6.final_layer_norm.weight", "2.model.encoder.layers.6.final_layer_norm.bias", "2.model.encoder.layers.7.attention.k_proj.weight", "2.model.encoder.layers.7.attention.k_proj.bias", "2.model.encoder.layers.7.attention.v_proj.weight", "2.model.encoder.layers.7.attention.v_proj.bias", "2.model.encoder.layers.7.attention.q_proj.weight", "2.model.encoder.layers.7.attention.q_proj.bias", "2.model.encoder.layers.7.attention.out_proj.weight", "2.model.encoder.layers.7.attention.out_proj.bias", "2.model.encoder.layers.7.layer_norm.weight", "2.model.encoder.layers.7.layer_norm.bias", "2.model.encoder.layers.7.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.7.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.7.feed_forward.output_dense.weight", "2.model.encoder.layers.7.feed_forward.output_dense.bias", "2.model.encoder.layers.7.final_layer_norm.weight", "2.model.encoder.layers.7.final_layer_norm.bias", "2.model.encoder.layers.8.attention.k_proj.weight", "2.model.encoder.layers.8.attention.k_proj.bias", "2.model.encoder.layers.8.attention.v_proj.weight", "2.model.encoder.layers.8.attention.v_proj.bias", "2.model.encoder.layers.8.attention.q_proj.weight", "2.model.encoder.layers.8.attention.q_proj.bias", "2.model.encoder.layers.8.attention.out_proj.weight", "2.model.encoder.layers.8.attention.out_proj.bias", "2.model.encoder.layers.8.layer_norm.weight", "2.model.encoder.layers.8.layer_norm.bias", "2.model.encoder.layers.8.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.8.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.8.feed_forward.output_dense.weight", "2.model.encoder.layers.8.feed_forward.output_dense.bias", "2.model.encoder.layers.8.final_layer_norm.weight", "2.model.encoder.layers.8.final_layer_norm.bias", "2.model.encoder.layers.9.attention.k_proj.weight", "2.model.encoder.layers.9.attention.k_proj.bias", "2.model.encoder.layers.9.attention.v_proj.weight", "2.model.encoder.layers.9.attention.v_proj.bias", "2.model.encoder.layers.9.attention.q_proj.weight", "2.model.encoder.layers.9.attention.q_proj.bias", "2.model.encoder.layers.9.attention.out_proj.weight", "2.model.encoder.layers.9.attention.out_proj.bias", "2.model.encoder.layers.9.layer_norm.weight", "2.model.encoder.layers.9.layer_norm.bias", "2.model.encoder.layers.9.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.9.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.9.feed_forward.output_dense.weight", "2.model.encoder.layers.9.feed_forward.output_dense.bias", "2.model.encoder.layers.9.final_layer_norm.weight", "2.model.encoder.layers.9.final_layer_norm.bias", "2.model.encoder.layers.10.attention.k_proj.weight", "2.model.encoder.layers.10.attention.k_proj.bias", "2.model.encoder.layers.10.attention.v_proj.weight", "2.model.encoder.layers.10.attention.v_proj.bias", "2.model.encoder.layers.10.attention.q_proj.weight", "2.model.encoder.layers.10.attention.q_proj.bias", "2.model.encoder.layers.10.attention.out_proj.weight", "2.model.encoder.layers.10.attention.out_proj.bias", "2.model.encoder.layers.10.layer_norm.weight", "2.model.encoder.layers.10.layer_norm.bias", "2.model.encoder.layers.10.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.10.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.10.feed_forward.output_dense.weight", "2.model.encoder.layers.10.feed_forward.output_dense.bias", "2.model.encoder.layers.10.final_layer_norm.weight", "2.model.encoder.layers.10.final_layer_norm.bias", "2.model.encoder.layers.11.attention.k_proj.weight", "2.model.encoder.layers.11.attention.k_proj.bias", "2.model.encoder.layers.11.attention.v_proj.weight", "2.model.encoder.layers.11.attention.v_proj.bias", "2.model.encoder.layers.11.attention.q_proj.weight", "2.model.encoder.layers.11.attention.q_proj.bias", "2.model.encoder.layers.11.attention.out_proj.weight", "2.model.encoder.layers.11.attention.out_proj.bias", "2.model.encoder.layers.11.layer_norm.weight", "2.model.encoder.layers.11.layer_norm.bias", "2.model.encoder.layers.11.feed_forward.intermediate_dense.weight", "2.model.encoder.layers.11.feed_forward.intermediate_dense.bias", "2.model.encoder.layers.11.feed_forward.output_dense.weight", "2.model.encoder.layers.11.feed_forward.output_dense.bias", "2.model.encoder.layers.11.final_layer_norm.weight", "2.model.encoder.layers.11.final_layer_norm.bias", "4.w.weight", "4.w.bias". 
	Unexpected key(s) in state_dict: "2.w.weight", "2.w.bias". 
2024-01-30 06:30:13,470 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:30:13,471 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:30:13,890 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:30:14,036 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:30:15,851 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:30:15,852 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:30:15,852 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:30:15,853 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:30:18,793 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:30:18,793 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:30:18,793 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:30:19,791 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:30:19,795 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:30:22,958 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
TypeError: Optimizer.load_state_dict() got an unexpected keyword argument 'strict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 98, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device))
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/optim/optimizer.py", line 390, in load_state_dict
    raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2024-01-30 06:30:58,717 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:30:58,718 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:30:59,119 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:30:59,327 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:31:01,179 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:31:01,179 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:31:01,179 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:31:01,180 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:31:03,815 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:31:03,816 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:31:03,816 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:31:04,218 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-30 06:31:04,221 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:31:07,802 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:31:16,121 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 541, in <module>
    asr_brain.evaluate(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1543, in evaluate
    loss = self.evaluate_batch(batch, stage=Stage.TEST)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 179, in evaluate_batch
    predictions = self.compute_forward(batch, stage=stage)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 86, in compute_forward
    enc_out, src_key_padding_mask = self.hparams.wav2vec2(src, wav_lens)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 315, in forward
    return self.extract_features(wav, wav_lens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 334, in extract_features
    out = self.model(
          ^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 1081, in forward
    extract_features = self.feature_extractor(input_values)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 354, in forward
    hidden_states = conv_layer(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 256, in forward
    hidden_states = self.conv(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 313, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 33, 20, 64]
2024-01-30 06:31:59,486 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:31:59,487 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:31:59,890 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:32:00,002 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:32:01,776 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:32:01,776 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:32:01,777 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:32:01,777 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:32:04,909 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:32:04,910 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:32:04,910 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:32:05,369 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-30 06:32:05,373 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:32:08,626 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:32:13,827 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 541, in <module>
    asr_brain.evaluate(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1543, in evaluate
    loss = self.evaluate_batch(batch, stage=Stage.TEST)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 179, in evaluate_batch
    predictions = self.compute_forward(batch, stage=stage)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 86, in compute_forward
    enc_out, src_key_padding_mask = self.hparams.wav2vec2(wavs, wav_lens)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 315, in forward
    return self.extract_features(wav, wav_lens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 334, in extract_features
    out = self.model(
          ^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 1081, in forward
    extract_features = self.feature_extractor(input_values)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 354, in forward
    hidden_states = conv_layer(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 256, in forward
    hidden_states = self.conv(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 313, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
2024-01-30 06:33:00,924 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:33:00,925 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:33:01,324 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:33:01,456 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:33:03,228 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:33:03,228 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:33:03,228 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:33:03,229 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:33:06,191 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:33:06,191 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:33:06,191 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:33:07,203 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:33:07,207 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:33:10,414 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
TypeError: Optimizer.load_state_dict() got an unexpected keyword argument 'strict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 98, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device))
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/optim/optimizer.py", line 390, in load_state_dict
    raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2024-01-30 06:35:41,141 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:35:41,142 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:35:41,557 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:35:41,657 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:35:43,402 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:35:43,402 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:35:43,403 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:35:43,403 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:35:46,308 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:35:46,309 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:35:46,309 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:35:47,347 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:35:47,352 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:35:49,327 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device))
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/optim/optimizer.py", line 390, in load_state_dict
    raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2024-01-30 06:51:18,416 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:51:18,417 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:51:18,827 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:51:19,030 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:51:20,877 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:51:20,877 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:51:20,878 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:51:20,878 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:51:23,523 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:51:23,523 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:51:23,523 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:51:24,448 - speechbrain.core - INFO - 165.9M trainable parameters in ASR
2024-01-30 06:51:24,452 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:51:27,359 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 96, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device), strict=True)
TypeError: Optimizer.load_state_dict() got an unexpected keyword argument 'strict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 525, in <module>
    asr_brain.fit(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1356, in fit
    self.on_fit_start()
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 939, in on_fit_start
    self.checkpointer.recover_if_possible(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 891, in recover_if_possible
    self.load_checkpoint(chosen_ckpt, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 904, in load_checkpoint
    self._call_load_hooks(checkpoint, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 1039, in _call_load_hooks
    default_hook(obj, loadpath, end_of_epoch, device)
  File "/home/rajivratn/satyam/speechbrain/speechbrain/utils/checkpoints.py", line 98, in torch_recovery
    obj.load_state_dict(torch.load(path, map_location=device))
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/optim/optimizer.py", line 390, in load_state_dict
    raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2024-01-30 06:54:20,234 - speechbrain.core - INFO - Beginning experiment!
2024-01-30 06:54:20,235 - speechbrain.core - INFO - Experiment folder: results/transformer/74443
2024-01-30 06:54:20,634 - speechbrain.utils.superpowers - DEBUG - Brotli @ file:///work/ci_py311/brotli-split_1676830125088/work
certifi @ file:///croot/certifi_1700501669400/work/certifi
cffi @ file:///croot/cffi_1700254295673/work
charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work
cryptography @ file:///croot/cryptography_1702070282333/work
filelock @ file:///croot/filelock_1700591183607/work
fsspec==2023.12.2
gmpy2 @ file:///work/ci_py311/gmpy2_1676839849213/work
huggingface-hub==0.20.2
HyperPyYAML==1.2.2
idna @ file:///work/ci_py311/idna_1676822698822/work
Jinja2 @ file:///work/ci_py311/jinja2_1676823587943/work
joblib==1.3.2
MarkupSafe @ file:///croot/markupsafe_1704205993651/work
mkl-fft @ file:///croot/mkl_fft_1695058164594/work
mkl-random @ file:///croot/mkl_random_1695059800811/work
mkl-service==2.4.0
mpmath @ file:///croot/mpmath_1690848262763/work
networkx @ file:///croot/networkx_1690561992265/work
numpy @ file:///croot/numpy_and_numpy_base_1704311704800/work/dist/numpy-1.26.3-cp311-cp311-linux_x86_64.whl#sha256=10a078151ecec16bafb535f7487635217625fa06536dec8509e514648c78d626
packaging==23.2
Pillow @ file:///croot/pillow_1696580024257/work
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyOpenSSL @ file:///croot/pyopenssl_1690223430423/work
PySocks @ file:///work/ci_py311/pysocks_1676822712504/work
PyYAML==6.0.1
regex==2023.12.25
requests @ file:///croot/requests_1690400202158/work
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
safetensors==0.4.2
scipy==1.11.4
sentencepiece==0.1.99
# Editable install with no version control (speechbrain==0.5.16)
-e /home/rajivratn/satyam/speechbrain
sympy @ file:///croot/sympy_1701397643339/work
tokenizers==0.15.1
torch==2.0.1
torchaudio==2.0.2
torchvision==0.15.2
tqdm==4.66.1
transformers==4.37.1
triton==2.0.0
typing_extensions @ file:///croot/typing_extensions_1705005625920/work
urllib3 @ file:///croot/urllib3_1698257533958/work


2024-01-30 06:54:20,756 - librispeech_prepare - INFO - Skipping preparation, completed in previous run.
2024-01-30 06:54:22,552 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results/transformer/74443/save.
2024-01-30 06:54:22,552 - speechbrain.pretrained.fetching - INFO - Fetch lm.ckpt: Using existing file/symlink in results/transformer/74443/save/lm.ckpt.
2024-01-30 06:54:22,552 - speechbrain.pretrained.fetching - INFO - Fetch tokenizer.ckpt: Using existing file/symlink in results/transformer/74443/save/tokenizer.ckpt.
2024-01-30 06:54:22,553 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2024-01-30 06:54:25,599 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2024-01-30 06:54:25,599 - speechbrain.core - INFO - Info: ckpt_interval_minutes arg from hparam file is used
2024-01-30 06:54:25,599 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2024-01-30 06:54:25,981 - speechbrain.core - INFO - 71.5M trainable parameters in ASR
2024-01-30 06:54:25,984 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:54:29,482 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results/transformer/74443/save/CKPT+2024-01-21+15-13-14+00
2024-01-30 06:54:35,115 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 544, in <module>
    asr_brain.evaluate(
  File "/home/rajivratn/satyam/speechbrain/speechbrain/core.py", line 1543, in evaluate
    loss = self.evaluate_batch(batch, stage=Stage.TEST)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 182, in evaluate_batch
    predictions = self.compute_forward(batch, stage=stage)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/recipes/LibriSpeech/ASR/transformer/train.py", line 86, in compute_forward
    enc_out, src_key_padding_mask = self.hparams.wav2vec2(wavs, wav_lens)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 315, in forward
    return self.extract_features(wav, wav_lens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/satyam/speechbrain/speechbrain/lobes/models/huggingface_wav2vec.py", line 334, in extract_features
    out = self.model(
          ^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 1081, in forward
    extract_features = self.feature_extractor(input_values)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 354, in forward
    hidden_states = conv_layer(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/transformers/models/hubert/modeling_hubert.py", line 256, in forward
    hidden_states = self.conv(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 313, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rajivratn/avinash/sb/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
